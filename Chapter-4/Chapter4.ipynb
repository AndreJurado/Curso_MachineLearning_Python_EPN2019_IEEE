{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src='images/Neuron.svg' width=60%\\>\n",
    "</center>\n",
    "\n",
    "[Fuente Wikipedia](https://commons.wikimedia.org/w/index.php?title=File:Neuron.svg&oldid=343028396)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una red neuronal es una estructura compuesta por **nodos** o **unidades** que se encuentran interconectados. La potencia de la interconexión entre los nodos se evalúa por medio un valor de **peso**. Si la suma ponderada de todas las conexiones al **nodo** o **neurona** es mayor que un **valor umbral**, decimos que la neurona se **activa**. La función matemática aplicada a la suma ponderada se denomina **función de activación**. \n",
    "\n",
    "Se denomina **Modelo de Perceptrón** a una red neuronal con una sola salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"174pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 174.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-257 170,-257 170,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"8,-8 8,-245 60,-245 60,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"106,-89 106,-164 158,-164 158,-89 106,-89\"/>\n",
       "<text text-anchor=\"middle\" x=\"132\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output</text>\n",
       "</g>\n",
       "<!-- x[0] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-196\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-192.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[0]</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"132\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"132\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M48.041,-184.3947C64.2693,-170.9815 91.1723,-148.7454 110.144,-133.0647\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.5069,-135.6525 117.985,-126.5838 108.0473,-130.2569 112.5069,-135.6525\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[0]</text>\n",
       "</g>\n",
       "<!-- x[1] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-142\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-138.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[1]</text>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.5204,-137.1729C66.2924,-133.1031 87.6838,-127.2096 104.6999,-122.5214\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.9201,-125.8158 114.6312,-119.7853 104.0608,-119.0672 105.9201,-125.8158\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-134.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[1]</text>\n",
       "</g>\n",
       "<!-- x[2] -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-88\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[2]</text>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.5204,-92.8271C66.2924,-96.8969 87.6838,-102.7904 104.6999,-107.4786\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.0608,-110.9328 114.6312,-110.2147 105.9201,-104.1842 104.0608,-110.9328\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-107.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[2]</text>\n",
       "</g>\n",
       "<!-- x[3] -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x[3]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-34\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[3]</text>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M48.041,-45.6053C64.2693,-59.0185 91.1723,-81.2546 110.144,-96.9353\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.0473,-99.7431 117.985,-103.4162 112.5069,-94.3475 108.0473,-99.7431\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-82.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">w[3]</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8a9d47b668>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_logistic_regression_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sea $\\mathbf{X}$ el espacio de entrada que contiene $N$ muestras de datos. Cada muestra está descrita por $d$ características o **features**. Sea $\\mathcal{Y} = \\{-1, +1\\}$ el espacio de salida binario. El perceptrón queda definido por:\n",
    "\n",
    "\n",
    "    \n",
    "$h(\\mathbf{x}) = sign \\left( \\sum_{i=1}^{d} \\left( w_ix_i \\right) + b \\right)$\n",
    "\n",
    "Donde **sign** es la función signo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sea $\\mathbf{w} = \\{w_{0},w_{1}, w_{2}, \\dots, w_{d} \\}^T$ el vector de pesos; en donde $w_{0}=b$ y sea $\\mathbf{x} = \\{x_{0},x_{1}, x_{2}, \\dots, x_{d} \\}^T $ con $w_{0}=1$, entonces la expresión para el Perceptron se puede reescribir:\n",
    "\n",
    "$h(\\mathbf{x}) = sign \\left(  \\mathbf{w}^T \\mathbf{x} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo de aprendizaje del perceptrón\n",
    "Para realizar el entrenamiento es preciso que las muestras de datos y etiquetas (o valores) sean randomizados. Luega en cada iteración se corregirá los valores del vector $\\mathbf{w}$, mediante la siguiente expresión:\n",
    "\n",
    "$\\mathbf{w}(t+1)=\\mathbf{w}(t)+\\alpha y(t)\\mathbf{x}(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El Perceptron  y la compuerta AND\n",
    "La compuerta **and** consta de 4 ejemplos. Cada uno con 2 características $d=2$. \n",
    "\n",
    "\n",
    "| muestra | x1 | x2 | y  |\n",
    "|---------|----|----|----|\n",
    "| 1       | -1 | -1 | -1 |\n",
    "| 2       | -1 |  1 | -1 |\n",
    "| 3       |  1 | -1 | -1 |\n",
    "| 4       |  1 | 1  |  1 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X es:\n",
      "[[-1 -1]\n",
      " [-1  1]\n",
      " [ 1 -1]\n",
      " [ 1  1]]\n",
      "Y es:\n",
      "[-1 -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "Y = np.array([-1, -1, -1, 1])\n",
    "print('X es:\\n{}'.format(X))\n",
    "print('Y es:\\n{}'.format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 4, Avg. loss: 0.750000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 8, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 12, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 16, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.41, NNZs: 2, Bias: -1.000000, T: 20, Avg. loss: 0.000000\n",
      "Total training time: 0.00 seconds.\n",
      "Rendimiento del entrenamiento: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "percept_and = Perceptron(verbose=1, shuffle=True)\n",
    "percept_and.fit(X, Y)\n",
    "print('Rendimiento del entrenamiento: {}'.format(percept_and.score(X,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = percept_and.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio:\n",
    "1. Verifique que la Compuerta **XOR** No tiene solución con un Perceptron\n",
    "2. Verifique que la Computerta **OR** Tiene solución con un Perceptrón\n",
    "3. Escriba un programa en Python que ejecute el algoritmo de aprendizaje del Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes Neuronales Multicapa con Alimentación hacia adelante (Feed Forward)\n",
    "Al añadir capas ocultas a una estructra de redes neuronales se amplía el espacio de hipótesis. Una red sencilla con una capa oculta con 3 perceptrones es la indicada en la siguiente figura. Observe la estructura de izquiera a derecha formada por:\n",
    "\n",
    "* Capa de entrada\n",
    "* Capa oculta\n",
    "* Capa de Salida\n",
    "\n",
    "Note que la capa de salida puede tener más nodos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"252pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 252.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-257 248,-257 248,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"8,-8 8,-245 60,-245 60,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_1</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"80,-35 80,-218 164,-218 164,-35 80,-35\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-202.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hidden layer</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"184,-89 184,-164 236,-164 236,-89 184,-89\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output</text>\n",
       "</g>\n",
       "<!-- x[0] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-196\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-192.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[0]</text>\n",
       "</g>\n",
       "<!-- h0 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>h0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122\" cy=\"-61\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h[0]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M43.9237,-180.7762C59.1082,-157.4817 88.244,-112.7848 106.311,-85.0684\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.4488,-86.6639 111.9776,-76.3753 103.5847,-82.8414 109.4488,-86.6639\"/>\n",
       "</g>\n",
       "<!-- h1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>h1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122\" cy=\"-169\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-165.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h[1]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-190.6657C63.8004,-186.8567 80.7424,-181.6586 94.9198,-177.3087\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.1167,-180.6026 104.6501,-174.3232 94.0634,-173.9105 96.1167,-180.6026\"/>\n",
       "</g>\n",
       "<!-- h2 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>h2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h[2]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-183.6978C61.5794,-170.6144 84.2611,-149.7369 100.8935,-134.4276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.5216,-136.7655 108.5089,-127.4179 98.7809,-131.6152 103.5216,-136.7655\"/>\n",
       "</g>\n",
       "<!-- x[1] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-142\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-138.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[1]</text>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-129.6978C61.5794,-116.6144 84.2611,-95.7369 100.8935,-80.4276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.5216,-82.7655 108.5089,-73.4179 98.7809,-77.6152 103.5216,-82.7655\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-147.3343C63.8004,-151.1433 80.7424,-156.3414 94.9198,-160.6913\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0634,-164.0895 104.6501,-163.6768 96.1167,-157.3974 94.0634,-164.0895\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-136.6657C63.8004,-132.8567 80.7424,-127.6586 94.9198,-123.3087\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.1167,-126.6026 104.6501,-120.3232 94.0634,-119.9105 96.1167,-126.6026\"/>\n",
       "</g>\n",
       "<!-- x[2] -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-88\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[2]</text>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h0 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-82.6657C63.8004,-78.8567 80.7424,-73.6586 94.9198,-69.3087\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.1167,-72.6026 104.6501,-66.3232 94.0634,-65.9105 96.1167,-72.6026\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-100.3022C61.5794,-113.3856 84.2611,-134.2631 100.8935,-149.5724\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.7809,-152.3848 108.5089,-156.5821 103.5216,-147.2345 98.7809,-152.3848\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h2 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-93.3343C63.8004,-97.1433 80.7424,-102.3414 94.9198,-106.6913\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0634,-110.0895 104.6501,-109.6768 96.1167,-103.3974 94.0634,-110.0895\"/>\n",
       "</g>\n",
       "<!-- x[3] -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x[3]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-34\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[3]</text>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h0 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.386,-39.3343C63.8004,-43.1433 80.7424,-48.3414 94.9198,-52.6913\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.0634,-56.0895 104.6501,-55.6768 96.1167,-49.3974 94.0634,-56.0895\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M43.9237,-49.2238C59.1082,-72.5183 88.244,-117.2152 106.311,-144.9316\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.5847,-147.1586 111.9776,-153.6247 109.4488,-143.3361 103.5847,-147.1586\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.3653,-46.3022C61.5794,-59.3856 84.2611,-80.2631 100.8935,-95.5724\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"98.7809,-98.3848 108.5089,-102.5821 103.5216,-93.2345 98.7809,-98.3848\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"210\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"210\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- h0&#45;&gt;y -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>h0&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.7326,-70.6541C150.9973,-78.7938 170.2058,-90.5808 185.4857,-99.9571\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.0498,-103.1824 194.4036,-105.4295 187.7109,-97.2161 184.0498,-103.1824\"/>\n",
       "</g>\n",
       "<!-- h1&#45;&gt;y -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>h1&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.7326,-159.3459C150.9973,-151.2062 170.2058,-139.4192 185.4857,-130.0429\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"187.7109,-132.7839 194.4036,-124.5705 184.0498,-126.8176 187.7109,-132.7839\"/>\n",
       "</g>\n",
       "<!-- h2&#45;&gt;y -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>h2&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.2337,-115C152.1508,-115 167.9616,-115 181.5183,-115\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"181.7897,-118.5001 191.7897,-115 181.7897,-111.5001 181.7897,-118.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8a46952c88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mglearn.plots.plot_single_hidden_layer_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La complejidad del problema puede requerir incrementar el número de capas ocultas. Para entrenar este tipo de redes se necesita el algoritmo de **back propagation** o retropropagación del error, ya que al tener varias neuronas en una capa oculta se dispone de un vector de hipótesis cuyos valores correctos no se conocen previamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"378pt\" height=\"261pt\"\n",
       " viewBox=\"0.00 0.00 378.00 261.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 257)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-257 374,-257 374,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"8,-8 8,-245 60,-245 60,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inputs</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_1</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"80,-35 80,-218 175,-218 175,-35 80,-35\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.5\" y=\"-202.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hidden layer 1</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"195,-35 195,-218 290,-218 290,-35 195,-35\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.5\" y=\"-202.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">hidden layer 2</text>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_3</title>\n",
       "<polygon fill=\"none\" stroke=\"#ffffff\" points=\"310,-89 310,-164 362,-164 362,-89 310,-89\"/>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output</text>\n",
       "</g>\n",
       "<!-- x[0] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-196\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-192.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[0]</text>\n",
       "</g>\n",
       "<!-- h1[0] -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>h1[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-61\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h1[0]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M44.308,-181.0368C60.4387,-157.6212 91.7762,-112.1314 110.9005,-84.3702\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"113.8551,-86.2509 116.6459,-76.0302 108.0905,-82.2797 113.8551,-86.2509\"/>\n",
       "</g>\n",
       "<!-- h1[1] -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>h1[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-169\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-165.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h1[1]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-190.9215C65.1319,-186.9617 84.3149,-181.3924 99.9321,-176.8584\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.9582,-180.2051 109.5858,-174.0557 99.0065,-173.4827 100.9582,-180.2051\"/>\n",
       "</g>\n",
       "<!-- h1[2] -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>h1[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"127\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h1[2]</text>\n",
       "</g>\n",
       "<!-- x[0]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x[0]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-184.0482C62.9704,-170.7677 87.7855,-149.1546 105.5924,-133.6453\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.9791,-136.208 113.2212,-127.0009 103.3816,-130.9294 107.9791,-136.208\"/>\n",
       "</g>\n",
       "<!-- x[1] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-142\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-138.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[1]</text>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-130.0482C62.9704,-116.7677 87.7855,-95.1546 105.5924,-79.6453\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"107.9791,-82.208 113.2212,-73.0009 103.3816,-76.9294 107.9791,-82.208\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-147.0785C65.1319,-151.0383 84.3149,-156.6076 99.9321,-161.1416\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.0065,-164.5173 109.5858,-163.9443 100.9582,-157.7949 99.0065,-164.5173\"/>\n",
       "</g>\n",
       "<!-- x[1]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>x[1]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-136.9215C65.1319,-132.9617 84.3149,-127.3924 99.9321,-122.8584\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.9582,-126.2051 109.5858,-120.0557 99.0065,-119.4827 100.9582,-126.2051\"/>\n",
       "</g>\n",
       "<!-- x[2] -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-88\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[2]</text>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-82.9215C65.1319,-78.9617 84.3149,-73.3924 99.9321,-68.8584\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.9582,-72.2051 109.5858,-66.0557 99.0065,-65.4827 100.9582,-72.2051\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-99.9518C62.9704,-113.2323 87.7855,-134.8454 105.5924,-150.3547\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3816,-153.0706 113.2212,-156.9991 107.9791,-147.792 103.3816,-153.0706\"/>\n",
       "</g>\n",
       "<!-- x[2]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>x[2]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-93.0785C65.1319,-97.0383 84.3149,-102.6076 99.9321,-107.1416\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.0065,-110.5173 109.5858,-109.9443 100.9582,-103.7949 99.0065,-110.5173\"/>\n",
       "</g>\n",
       "<!-- x[3] -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x[3]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"34\" cy=\"-34\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"34\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x[3]</text>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1[0] -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.4926,-39.0785C65.1319,-43.0383 84.3149,-48.6076 99.9321,-53.1416\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.0065,-56.5173 109.5858,-55.9443 100.9582,-49.7949 99.0065,-56.5173\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1[1] -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M44.308,-48.9632C60.4387,-72.3788 91.7762,-117.8686 110.9005,-145.6298\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.0905,-147.7203 116.6459,-153.9698 113.8551,-143.7491 108.0905,-147.7203\"/>\n",
       "</g>\n",
       "<!-- x[3]&#45;&gt;h1[2] -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>x[3]&#45;&gt;h1[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.7225,-45.9518C62.9704,-59.2323 87.7855,-80.8454 105.5924,-96.3547\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.3816,-99.0706 113.2212,-102.9991 107.9791,-93.792 103.3816,-99.0706\"/>\n",
       "</g>\n",
       "<!-- h2[0] -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>h2[0]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"242\" cy=\"-61\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242\" y=\"-57.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h2[0]</text>\n",
       "</g>\n",
       "<!-- h1[0]&#45;&gt;h2[0] -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>h1[0]&#45;&gt;h2[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.2221,-61C163.6433,-61 192.3671,-61 213.7431,-61\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.8632,-64.5001 223.8632,-61 213.8631,-57.5001 213.8632,-64.5001\"/>\n",
       "</g>\n",
       "<!-- h2[1] -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>h2[1]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"242\" cy=\"-169\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242\" y=\"-165.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h2[1]</text>\n",
       "</g>\n",
       "<!-- h1[0]&#45;&gt;h2[1] -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>h1[0]&#45;&gt;h2[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.192,-73.389C160.0161,-92.0064 197.669,-127.3674 221.211,-149.4764\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"218.9222,-152.1284 228.6077,-156.4228 223.7142,-147.0258 218.9222,-152.1284\"/>\n",
       "</g>\n",
       "<!-- h2[2] -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>h2[2]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"242\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">h2[2]</text>\n",
       "</g>\n",
       "<!-- h1[0]&#45;&gt;h2[2] -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>h1[0]&#45;&gt;h2[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-68.7369C162.5201,-77.679 194.0899,-92.5031 216.3526,-102.9569\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.0145,-106.1951 225.5539,-107.2775 217.9898,-99.8589 215.0145,-106.1951\"/>\n",
       "</g>\n",
       "<!-- h1[1]&#45;&gt;h2[0] -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>h1[1]&#45;&gt;h2[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.192,-156.611C160.0161,-137.9936 197.669,-102.6326 221.211,-80.5236\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"223.7142,-82.9742 228.6077,-73.5772 218.9222,-77.8716 223.7142,-82.9742\"/>\n",
       "</g>\n",
       "<!-- h1[1]&#45;&gt;h2[1] -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>h1[1]&#45;&gt;h2[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.2221,-169C163.6433,-169 192.3671,-169 213.7431,-169\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.8632,-172.5001 223.8632,-169 213.8631,-165.5001 213.8632,-172.5001\"/>\n",
       "</g>\n",
       "<!-- h1[1]&#45;&gt;h2[2] -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>h1[1]&#45;&gt;h2[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-161.2631C162.5201,-152.321 194.0899,-137.4969 216.3526,-127.0431\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.9898,-130.1411 225.5539,-122.7225 215.0145,-123.8049 217.9898,-130.1411\"/>\n",
       "</g>\n",
       "<!-- h1[2]&#45;&gt;h2[0] -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>h1[2]&#45;&gt;h2[0]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-107.2631C162.5201,-98.321 194.0899,-83.4969 216.3526,-73.0431\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.9898,-76.1411 225.5539,-68.7225 215.0145,-69.8049 217.9898,-76.1411\"/>\n",
       "</g>\n",
       "<!-- h1[2]&#45;&gt;h2[1] -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>h1[2]&#45;&gt;h2[1]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4767,-122.7369C162.5201,-131.679 194.0899,-146.5031 216.3526,-156.9569\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.0145,-160.1951 225.5539,-161.2775 217.9898,-153.8589 215.0145,-160.1951\"/>\n",
       "</g>\n",
       "<!-- h1[2]&#45;&gt;h2[2] -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>h1[2]&#45;&gt;h2[2]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.2221,-115C163.6433,-115 192.3671,-115 213.7431,-115\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"213.8632,-118.5001 223.8632,-115 213.8631,-111.5001 213.8632,-118.5001\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"336\" cy=\"-115\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-111.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- h2[0]&#45;&gt;y -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>h2[0]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M257.9458,-70.1604C272.6188,-78.5895 294.6007,-91.2174 311.5017,-100.9265\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"309.8752,-104.0285 320.2897,-105.9749 313.3621,-97.9588 309.8752,-104.0285\"/>\n",
       "</g>\n",
       "<!-- h2[1]&#45;&gt;y -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>h2[1]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M257.9458,-159.8396C272.6188,-151.4105 294.6007,-138.7826 311.5017,-129.0735\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"313.3621,-132.0412 320.2897,-124.0251 309.8752,-125.9715 313.3621,-132.0412\"/>\n",
       "</g>\n",
       "<!-- h2[2]&#45;&gt;y -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>h2[2]&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.1241,-115C273.6484,-115 292.3808,-115 307.8486,-115\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"307.9315,-118.5001 317.9315,-115 307.9315,-111.5001 307.9315,-118.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8a46905160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mglearn.plots.plot_two_hidden_layer_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio:\n",
    "Utilice una red neuronal multicapa para clasificar los datos del dataset moons, con:\n",
    "* Algoritmo de descenso de gradiente estocástico,\n",
    "* 50 neuronas en la capa oculta,\n",
    "* Función de activación relu, luego con sigmoid\n",
    "* Pruebe con algoritmo lbfgs y sgd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
      " 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "# print(X[0:5,:])\n",
    "print(y)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Realicemos un plot de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y, cmap=cm_bright)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67948739\n",
      "Iteration 2, loss = 0.67915809\n",
      "Iteration 3, loss = 0.67868928\n",
      "Iteration 4, loss = 0.67809569\n",
      "Iteration 5, loss = 0.67739037\n",
      "Iteration 6, loss = 0.67658609\n",
      "Iteration 7, loss = 0.67569401\n",
      "Iteration 8, loss = 0.67472395\n",
      "Iteration 9, loss = 0.67368554\n",
      "Iteration 10, loss = 0.67258706\n",
      "Iteration 11, loss = 0.67143620\n",
      "Iteration 12, loss = 0.67023946\n",
      "Iteration 13, loss = 0.66900273\n",
      "Iteration 14, loss = 0.66773188\n",
      "Iteration 15, loss = 0.66643203\n",
      "Iteration 16, loss = 0.66510874\n",
      "Iteration 17, loss = 0.66376526\n",
      "Iteration 18, loss = 0.66240714\n",
      "Iteration 19, loss = 0.66103601\n",
      "Iteration 20, loss = 0.65965500\n",
      "Iteration 21, loss = 0.65826646\n",
      "Iteration 22, loss = 0.65687314\n",
      "Iteration 23, loss = 0.65548093\n",
      "Iteration 24, loss = 0.65408727\n",
      "Iteration 25, loss = 0.65269483\n",
      "Iteration 26, loss = 0.65130382\n",
      "Iteration 27, loss = 0.64991403\n",
      "Iteration 28, loss = 0.64852883\n",
      "Iteration 29, loss = 0.64714910\n",
      "Iteration 30, loss = 0.64577370\n",
      "Iteration 31, loss = 0.64440392\n",
      "Iteration 32, loss = 0.64304056\n",
      "Iteration 33, loss = 0.64168409\n",
      "Iteration 34, loss = 0.64033565\n",
      "Iteration 35, loss = 0.63899685\n",
      "Iteration 36, loss = 0.63766786\n",
      "Iteration 37, loss = 0.63634986\n",
      "Iteration 38, loss = 0.63504046\n",
      "Iteration 39, loss = 0.63374092\n",
      "Iteration 40, loss = 0.63245082\n",
      "Iteration 41, loss = 0.63117028\n",
      "Iteration 42, loss = 0.62989938\n",
      "Iteration 43, loss = 0.62863815\n",
      "Iteration 44, loss = 0.62738664\n",
      "Iteration 45, loss = 0.62614479\n",
      "Iteration 46, loss = 0.62491189\n",
      "Iteration 47, loss = 0.62368774\n",
      "Iteration 48, loss = 0.62247331\n",
      "Iteration 49, loss = 0.62126771\n",
      "Iteration 50, loss = 0.62007155\n",
      "Iteration 51, loss = 0.61888572\n",
      "Iteration 52, loss = 0.61771018\n",
      "Iteration 53, loss = 0.61654400\n",
      "Iteration 54, loss = 0.61538845\n",
      "Iteration 55, loss = 0.61424232\n",
      "Iteration 56, loss = 0.61310339\n",
      "Iteration 57, loss = 0.61197311\n",
      "Iteration 58, loss = 0.61085129\n",
      "Iteration 59, loss = 0.60973756\n",
      "Iteration 60, loss = 0.60863332\n",
      "Iteration 61, loss = 0.60753848\n",
      "Iteration 62, loss = 0.60645568\n",
      "Iteration 63, loss = 0.60538199\n",
      "Iteration 64, loss = 0.60431628\n",
      "Iteration 65, loss = 0.60325857\n",
      "Iteration 66, loss = 0.60220887\n",
      "Iteration 67, loss = 0.60116726\n",
      "Iteration 68, loss = 0.60013325\n",
      "Iteration 69, loss = 0.59910661\n",
      "Iteration 70, loss = 0.59808737\n",
      "Iteration 71, loss = 0.59707484\n",
      "Iteration 72, loss = 0.59606986\n",
      "Iteration 73, loss = 0.59507221\n",
      "Iteration 74, loss = 0.59408181\n",
      "Iteration 75, loss = 0.59309812\n",
      "Iteration 76, loss = 0.59212093\n",
      "Iteration 77, loss = 0.59115095\n",
      "Iteration 78, loss = 0.59018997\n",
      "Iteration 79, loss = 0.58923588\n",
      "Iteration 80, loss = 0.58828744\n",
      "Iteration 81, loss = 0.58734535\n",
      "Iteration 82, loss = 0.58640980\n",
      "Iteration 83, loss = 0.58548100\n",
      "Iteration 84, loss = 0.58455817\n",
      "Iteration 85, loss = 0.58364056\n",
      "Iteration 86, loss = 0.58273002\n",
      "Iteration 87, loss = 0.58182562\n",
      "Iteration 88, loss = 0.58092783\n",
      "Iteration 89, loss = 0.58003610\n",
      "Iteration 90, loss = 0.57915074\n",
      "Iteration 91, loss = 0.57827187\n",
      "Iteration 92, loss = 0.57739884\n",
      "Iteration 93, loss = 0.57653159\n",
      "Iteration 94, loss = 0.57567015\n",
      "Iteration 95, loss = 0.57481401\n",
      "Iteration 96, loss = 0.57396338\n",
      "Iteration 97, loss = 0.57311730\n",
      "Iteration 98, loss = 0.57227617\n",
      "Iteration 99, loss = 0.57143938\n",
      "Iteration 100, loss = 0.57060728\n",
      "Iteration 101, loss = 0.56978023\n",
      "Iteration 102, loss = 0.56895875\n",
      "Iteration 103, loss = 0.56814203\n",
      "Iteration 104, loss = 0.56732996\n",
      "Iteration 105, loss = 0.56652201\n",
      "Iteration 106, loss = 0.56571803\n",
      "Iteration 107, loss = 0.56491839\n",
      "Iteration 108, loss = 0.56412349\n",
      "Iteration 109, loss = 0.56333252\n",
      "Iteration 110, loss = 0.56254547\n",
      "Iteration 111, loss = 0.56176196\n",
      "Iteration 112, loss = 0.56098270\n",
      "Iteration 113, loss = 0.56020806\n",
      "Iteration 114, loss = 0.55943775\n",
      "Iteration 115, loss = 0.55867174\n",
      "Iteration 116, loss = 0.55791036\n",
      "Iteration 117, loss = 0.55715305\n",
      "Iteration 118, loss = 0.55639927\n",
      "Iteration 119, loss = 0.55564879\n",
      "Iteration 120, loss = 0.55490268\n",
      "Iteration 121, loss = 0.55416094\n",
      "Iteration 122, loss = 0.55342304\n",
      "Iteration 123, loss = 0.55268978\n",
      "Iteration 124, loss = 0.55196093\n",
      "Iteration 125, loss = 0.55123603\n",
      "Iteration 126, loss = 0.55051513\n",
      "Iteration 127, loss = 0.54979818\n",
      "Iteration 128, loss = 0.54908516\n",
      "Iteration 129, loss = 0.54837510\n",
      "Iteration 130, loss = 0.54766853\n",
      "Iteration 131, loss = 0.54696538\n",
      "Iteration 132, loss = 0.54626561\n",
      "Iteration 133, loss = 0.54556928\n",
      "Iteration 134, loss = 0.54487656\n",
      "Iteration 135, loss = 0.54418738\n",
      "Iteration 136, loss = 0.54350124\n",
      "Iteration 137, loss = 0.54281880\n",
      "Iteration 138, loss = 0.54214022\n",
      "Iteration 139, loss = 0.54146561\n",
      "Iteration 140, loss = 0.54079514\n",
      "Iteration 141, loss = 0.54012856\n",
      "Iteration 142, loss = 0.53946562\n",
      "Iteration 143, loss = 0.53880617\n",
      "Iteration 144, loss = 0.53815011\n",
      "Iteration 145, loss = 0.53749743\n",
      "Iteration 146, loss = 0.53684822\n",
      "Iteration 147, loss = 0.53620310\n",
      "Iteration 148, loss = 0.53556167\n",
      "Iteration 149, loss = 0.53492410\n",
      "Iteration 150, loss = 0.53429046\n",
      "Iteration 151, loss = 0.53365973\n",
      "Iteration 152, loss = 0.53303211\n",
      "Iteration 153, loss = 0.53240757\n",
      "Iteration 154, loss = 0.53178585\n",
      "Iteration 155, loss = 0.53116653\n",
      "Iteration 156, loss = 0.53055019\n",
      "Iteration 157, loss = 0.52993690\n",
      "Iteration 158, loss = 0.52932672\n",
      "Iteration 159, loss = 0.52871987\n",
      "Iteration 160, loss = 0.52811606\n",
      "Iteration 161, loss = 0.52751519\n",
      "Iteration 162, loss = 0.52691782\n",
      "Iteration 163, loss = 0.52632434\n",
      "Iteration 164, loss = 0.52573337\n",
      "Iteration 165, loss = 0.52514639\n",
      "Iteration 166, loss = 0.52456305\n",
      "Iteration 167, loss = 0.52398200\n",
      "Iteration 168, loss = 0.52340298\n",
      "Iteration 169, loss = 0.52282711\n",
      "Iteration 170, loss = 0.52225508\n",
      "Iteration 171, loss = 0.52168600\n",
      "Iteration 172, loss = 0.52111966\n",
      "Iteration 173, loss = 0.52055644\n",
      "Iteration 174, loss = 0.51999601\n",
      "Iteration 175, loss = 0.51943866\n",
      "Iteration 176, loss = 0.51888397\n",
      "Iteration 177, loss = 0.51833193\n",
      "Iteration 178, loss = 0.51778249\n",
      "Iteration 179, loss = 0.51723564\n",
      "Iteration 180, loss = 0.51669135\n",
      "Iteration 181, loss = 0.51615009\n",
      "Iteration 182, loss = 0.51561203\n",
      "Iteration 183, loss = 0.51507645\n",
      "Iteration 184, loss = 0.51454417\n",
      "Iteration 185, loss = 0.51401440\n",
      "Iteration 186, loss = 0.51348713\n",
      "Iteration 187, loss = 0.51296232\n",
      "Iteration 188, loss = 0.51244007\n",
      "Iteration 189, loss = 0.51192029\n",
      "Iteration 190, loss = 0.51140290\n",
      "Iteration 191, loss = 0.51088798\n",
      "Iteration 192, loss = 0.51037559\n",
      "Iteration 193, loss = 0.50986578\n",
      "Iteration 194, loss = 0.50935834\n",
      "Iteration 195, loss = 0.50885345\n",
      "Iteration 196, loss = 0.50835003\n",
      "Iteration 197, loss = 0.50784895\n",
      "Iteration 198, loss = 0.50734997\n",
      "Iteration 199, loss = 0.50685322\n",
      "Iteration 200, loss = 0.50635846\n",
      "Iteration 201, loss = 0.50586568\n",
      "Iteration 202, loss = 0.50537497\n",
      "Iteration 203, loss = 0.50488630\n",
      "Iteration 204, loss = 0.50439981\n",
      "Iteration 205, loss = 0.50391539\n",
      "Iteration 206, loss = 0.50343300\n",
      "Iteration 207, loss = 0.50295250\n",
      "Iteration 208, loss = 0.50247438\n",
      "Iteration 209, loss = 0.50199837\n",
      "Iteration 210, loss = 0.50152423\n",
      "Iteration 211, loss = 0.50105137\n",
      "Iteration 212, loss = 0.50058033\n",
      "Iteration 213, loss = 0.50011117\n",
      "Iteration 214, loss = 0.49964392\n",
      "Iteration 215, loss = 0.49917861\n",
      "Iteration 216, loss = 0.49871487\n",
      "Iteration 217, loss = 0.49825310\n",
      "Iteration 218, loss = 0.49779334\n",
      "Iteration 219, loss = 0.49733559\n",
      "Iteration 220, loss = 0.49688009\n",
      "Iteration 221, loss = 0.49642652\n",
      "Iteration 222, loss = 0.49597481\n",
      "Iteration 223, loss = 0.49552494\n",
      "Iteration 224, loss = 0.49507723\n",
      "Iteration 225, loss = 0.49463174\n",
      "Iteration 226, loss = 0.49418847\n",
      "Iteration 227, loss = 0.49374708\n",
      "Iteration 228, loss = 0.49330771\n",
      "Iteration 229, loss = 0.49287017\n",
      "Iteration 230, loss = 0.49243444\n",
      "Iteration 231, loss = 0.49200049\n",
      "Iteration 232, loss = 0.49156847\n",
      "Iteration 233, loss = 0.49113822\n",
      "Iteration 234, loss = 0.49070925\n",
      "Iteration 235, loss = 0.49028184\n",
      "Iteration 236, loss = 0.48985610\n",
      "Iteration 237, loss = 0.48943239\n",
      "Iteration 238, loss = 0.48901071\n",
      "Iteration 239, loss = 0.48859072\n",
      "Iteration 240, loss = 0.48817253\n",
      "Iteration 241, loss = 0.48775661\n",
      "Iteration 242, loss = 0.48734245\n",
      "Iteration 243, loss = 0.48693016\n",
      "Iteration 244, loss = 0.48651966\n",
      "Iteration 245, loss = 0.48611105\n",
      "Iteration 246, loss = 0.48570401\n",
      "Iteration 247, loss = 0.48529852\n",
      "Iteration 248, loss = 0.48489465\n",
      "Iteration 249, loss = 0.48449275\n",
      "Iteration 250, loss = 0.48409258\n",
      "Iteration 251, loss = 0.48369403\n",
      "Iteration 252, loss = 0.48329707\n",
      "Iteration 253, loss = 0.48290181\n",
      "Iteration 254, loss = 0.48250819\n",
      "Iteration 255, loss = 0.48211618\n",
      "Iteration 256, loss = 0.48172580\n",
      "Iteration 257, loss = 0.48133740\n",
      "Iteration 258, loss = 0.48095090\n",
      "Iteration 259, loss = 0.48056597\n",
      "Iteration 260, loss = 0.48018257\n",
      "Iteration 261, loss = 0.47980071\n",
      "Iteration 262, loss = 0.47942038\n",
      "Iteration 263, loss = 0.47904159\n",
      "Iteration 264, loss = 0.47866429\n",
      "Iteration 265, loss = 0.47828848\n",
      "Iteration 266, loss = 0.47791413\n",
      "Iteration 267, loss = 0.47754124\n",
      "Iteration 268, loss = 0.47716994\n",
      "Iteration 269, loss = 0.47680011\n",
      "Iteration 270, loss = 0.47643171\n",
      "Iteration 271, loss = 0.47606487\n",
      "Iteration 272, loss = 0.47569950\n",
      "Iteration 273, loss = 0.47533559\n",
      "Iteration 274, loss = 0.47497323\n",
      "Iteration 275, loss = 0.47461209\n",
      "Iteration 276, loss = 0.47425227\n",
      "Iteration 277, loss = 0.47389381\n",
      "Iteration 278, loss = 0.47353670\n",
      "Iteration 279, loss = 0.47318108\n",
      "Iteration 280, loss = 0.47282687\n",
      "Iteration 281, loss = 0.47247399\n",
      "Iteration 282, loss = 0.47212245\n",
      "Iteration 283, loss = 0.47177237\n",
      "Iteration 284, loss = 0.47142364\n",
      "Iteration 285, loss = 0.47107623\n",
      "Iteration 286, loss = 0.47073031\n",
      "Iteration 287, loss = 0.47038570\n",
      "Iteration 288, loss = 0.47004243\n",
      "Iteration 289, loss = 0.46970070\n",
      "Iteration 290, loss = 0.46936025\n",
      "Iteration 291, loss = 0.46902103\n",
      "Iteration 292, loss = 0.46868301\n",
      "Iteration 293, loss = 0.46834633\n",
      "Iteration 294, loss = 0.46801094\n",
      "Iteration 295, loss = 0.46767678\n",
      "Iteration 296, loss = 0.46734386\n",
      "Iteration 297, loss = 0.46701215\n",
      "Iteration 298, loss = 0.46668166\n",
      "Iteration 299, loss = 0.46635233\n",
      "Iteration 300, loss = 0.46602385\n",
      "Iteration 301, loss = 0.46569654\n",
      "Iteration 302, loss = 0.46537040\n",
      "Iteration 303, loss = 0.46504550\n",
      "Iteration 304, loss = 0.46472182\n",
      "Iteration 305, loss = 0.46439939\n",
      "Iteration 306, loss = 0.46407811\n",
      "Iteration 307, loss = 0.46375800\n",
      "Iteration 308, loss = 0.46343904\n",
      "Iteration 309, loss = 0.46312123\n",
      "Iteration 310, loss = 0.46280456\n",
      "Iteration 311, loss = 0.46248903\n",
      "Iteration 312, loss = 0.46217468\n",
      "Iteration 313, loss = 0.46186155\n",
      "Iteration 314, loss = 0.46154955\n",
      "Iteration 315, loss = 0.46123874\n",
      "Iteration 316, loss = 0.46092915\n",
      "Iteration 317, loss = 0.46062074\n",
      "Iteration 318, loss = 0.46031385\n",
      "Iteration 319, loss = 0.46000837\n",
      "Iteration 320, loss = 0.45970403\n",
      "Iteration 321, loss = 0.45940089\n",
      "Iteration 322, loss = 0.45909896\n",
      "Iteration 323, loss = 0.45879818\n",
      "Iteration 324, loss = 0.45849851\n",
      "Iteration 325, loss = 0.45819995\n",
      "Iteration 326, loss = 0.45790248\n",
      "Iteration 327, loss = 0.45760611\n",
      "Iteration 328, loss = 0.45731082\n",
      "Iteration 329, loss = 0.45701660\n",
      "Iteration 330, loss = 0.45672345\n",
      "Iteration 331, loss = 0.45643136\n",
      "Iteration 332, loss = 0.45614036\n",
      "Iteration 333, loss = 0.45585051\n",
      "Iteration 334, loss = 0.45556149\n",
      "Iteration 335, loss = 0.45527331\n",
      "Iteration 336, loss = 0.45498612\n",
      "Iteration 337, loss = 0.45469994\n",
      "Iteration 338, loss = 0.45441474\n",
      "Iteration 339, loss = 0.45413055\n",
      "Iteration 340, loss = 0.45384733\n",
      "Iteration 341, loss = 0.45356511\n",
      "Iteration 342, loss = 0.45328394\n",
      "Iteration 343, loss = 0.45300394\n",
      "Iteration 344, loss = 0.45272492\n",
      "Iteration 345, loss = 0.45244688\n",
      "Iteration 346, loss = 0.45216982\n",
      "Iteration 347, loss = 0.45189343\n",
      "Iteration 348, loss = 0.45161791\n",
      "Iteration 349, loss = 0.45134332\n",
      "Iteration 350, loss = 0.45106967\n",
      "Iteration 351, loss = 0.45079694\n",
      "Iteration 352, loss = 0.45052494\n",
      "Iteration 353, loss = 0.45025374\n",
      "Iteration 354, loss = 0.44998342\n",
      "Iteration 355, loss = 0.44971405\n",
      "Iteration 356, loss = 0.44944561\n",
      "Iteration 357, loss = 0.44917807\n",
      "Iteration 358, loss = 0.44891143\n",
      "Iteration 359, loss = 0.44864557\n",
      "Iteration 360, loss = 0.44838055\n",
      "Iteration 361, loss = 0.44811647\n",
      "Iteration 362, loss = 0.44785329\n",
      "Iteration 363, loss = 0.44759098\n",
      "Iteration 364, loss = 0.44732965\n",
      "Iteration 365, loss = 0.44706931\n",
      "Iteration 366, loss = 0.44680997\n",
      "Iteration 367, loss = 0.44655160\n",
      "Iteration 368, loss = 0.44629410\n",
      "Iteration 369, loss = 0.44603745\n",
      "Iteration 370, loss = 0.44578175\n",
      "Iteration 371, loss = 0.44552704\n",
      "Iteration 372, loss = 0.44527321\n",
      "Iteration 373, loss = 0.44502030\n",
      "Iteration 374, loss = 0.44476855\n",
      "Iteration 375, loss = 0.44451767\n",
      "Iteration 376, loss = 0.44426775\n",
      "Iteration 377, loss = 0.44401866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 378, loss = 0.44377026\n",
      "Iteration 379, loss = 0.44352269\n",
      "Iteration 380, loss = 0.44327597\n",
      "Iteration 381, loss = 0.44303007\n",
      "Iteration 382, loss = 0.44278501\n",
      "Iteration 383, loss = 0.44254077\n",
      "Iteration 384, loss = 0.44229735\n",
      "Iteration 385, loss = 0.44205475\n",
      "Iteration 386, loss = 0.44181297\n",
      "Iteration 387, loss = 0.44157196\n",
      "Iteration 388, loss = 0.44133173\n",
      "Iteration 389, loss = 0.44109229\n",
      "Iteration 390, loss = 0.44085365\n",
      "Iteration 391, loss = 0.44061580\n",
      "Iteration 392, loss = 0.44037874\n",
      "Iteration 393, loss = 0.44014246\n",
      "Iteration 394, loss = 0.43990697\n",
      "Iteration 395, loss = 0.43967226\n",
      "Iteration 396, loss = 0.43943822\n",
      "Iteration 397, loss = 0.43920498\n",
      "Iteration 398, loss = 0.43897324\n",
      "Iteration 399, loss = 0.43874337\n",
      "Iteration 400, loss = 0.43851436\n",
      "Iteration 401, loss = 0.43828613\n",
      "Iteration 402, loss = 0.43805854\n",
      "Iteration 403, loss = 0.43783175\n",
      "Iteration 404, loss = 0.43760575\n",
      "Iteration 405, loss = 0.43738054\n",
      "Iteration 406, loss = 0.43715614\n",
      "Iteration 407, loss = 0.43693252\n",
      "Iteration 408, loss = 0.43670985\n",
      "Iteration 409, loss = 0.43648799\n",
      "Iteration 410, loss = 0.43626689\n",
      "Iteration 411, loss = 0.43604655\n",
      "Iteration 412, loss = 0.43582697\n",
      "Iteration 413, loss = 0.43560813\n",
      "Iteration 414, loss = 0.43539004\n",
      "Iteration 415, loss = 0.43517269\n",
      "Iteration 416, loss = 0.43495606\n",
      "Iteration 417, loss = 0.43473998\n",
      "Iteration 418, loss = 0.43452459\n",
      "Iteration 419, loss = 0.43430988\n",
      "Iteration 420, loss = 0.43409563\n",
      "Iteration 421, loss = 0.43388189\n",
      "Iteration 422, loss = 0.43366866\n",
      "Iteration 423, loss = 0.43345609\n",
      "Iteration 424, loss = 0.43324418\n",
      "Iteration 425, loss = 0.43303298\n",
      "Iteration 426, loss = 0.43282249\n",
      "Iteration 427, loss = 0.43261266\n",
      "Iteration 428, loss = 0.43240349\n",
      "Iteration 429, loss = 0.43219498\n",
      "Iteration 430, loss = 0.43198713\n",
      "Iteration 431, loss = 0.43177998\n",
      "Iteration 432, loss = 0.43157338\n",
      "Iteration 433, loss = 0.43136742\n",
      "Iteration 434, loss = 0.43116212\n",
      "Iteration 435, loss = 0.43095743\n",
      "Iteration 436, loss = 0.43075313\n",
      "Iteration 437, loss = 0.43054944\n",
      "Iteration 438, loss = 0.43034638\n",
      "Iteration 439, loss = 0.43014394\n",
      "Iteration 440, loss = 0.42994211\n",
      "Iteration 441, loss = 0.42974091\n",
      "Iteration 442, loss = 0.42954041\n",
      "Iteration 443, loss = 0.42934069\n",
      "Iteration 444, loss = 0.42914176\n",
      "Iteration 445, loss = 0.42894347\n",
      "Iteration 446, loss = 0.42874581\n",
      "Iteration 447, loss = 0.42854878\n",
      "Iteration 448, loss = 0.42835237\n",
      "Iteration 449, loss = 0.42815666\n",
      "Iteration 450, loss = 0.42796171\n",
      "Iteration 451, loss = 0.42776739\n",
      "Iteration 452, loss = 0.42757396\n",
      "Iteration 453, loss = 0.42738132\n",
      "Iteration 454, loss = 0.42718941\n",
      "Iteration 455, loss = 0.42699807\n",
      "Iteration 456, loss = 0.42680734\n",
      "Iteration 457, loss = 0.42661706\n",
      "Iteration 458, loss = 0.42642728\n",
      "Iteration 459, loss = 0.42623809\n",
      "Iteration 460, loss = 0.42604948\n",
      "Iteration 461, loss = 0.42586145\n",
      "Iteration 462, loss = 0.42567401\n",
      "Iteration 463, loss = 0.42548715\n",
      "Iteration 464, loss = 0.42530078\n",
      "Iteration 465, loss = 0.42511482\n",
      "Iteration 466, loss = 0.42492942\n",
      "Iteration 467, loss = 0.42474457\n",
      "Iteration 468, loss = 0.42456027\n",
      "Iteration 469, loss = 0.42437653\n",
      "Iteration 470, loss = 0.42419334\n",
      "Iteration 471, loss = 0.42401070\n",
      "Iteration 472, loss = 0.42382866\n",
      "Iteration 473, loss = 0.42364722\n",
      "Iteration 474, loss = 0.42346643\n",
      "Iteration 475, loss = 0.42328619\n",
      "Iteration 476, loss = 0.42310651\n",
      "Iteration 477, loss = 0.42292738\n",
      "Iteration 478, loss = 0.42274879\n",
      "Iteration 479, loss = 0.42257076\n",
      "Iteration 480, loss = 0.42239326\n",
      "Iteration 481, loss = 0.42221631\n",
      "Iteration 482, loss = 0.42203974\n",
      "Iteration 483, loss = 0.42186346\n",
      "Iteration 484, loss = 0.42168768\n",
      "Iteration 485, loss = 0.42151240\n",
      "Iteration 486, loss = 0.42133764\n",
      "Iteration 487, loss = 0.42116337\n",
      "Iteration 488, loss = 0.42098962\n",
      "Iteration 489, loss = 0.42081636\n",
      "Iteration 490, loss = 0.42064375\n",
      "Iteration 491, loss = 0.42047168\n",
      "Iteration 492, loss = 0.42030030\n",
      "Iteration 493, loss = 0.42012944\n",
      "Iteration 494, loss = 0.41995914\n",
      "Iteration 495, loss = 0.41978936\n",
      "Iteration 496, loss = 0.41962009\n",
      "Iteration 497, loss = 0.41945133\n",
      "Iteration 498, loss = 0.41928309\n",
      "Iteration 499, loss = 0.41911535\n",
      "Iteration 500, loss = 0.41894811\n",
      "Iteration 501, loss = 0.41878139\n",
      "Iteration 502, loss = 0.41861519\n",
      "Iteration 503, loss = 0.41844950\n",
      "Iteration 504, loss = 0.41828412\n",
      "Iteration 505, loss = 0.41811906\n",
      "Iteration 506, loss = 0.41795437\n",
      "Iteration 507, loss = 0.41779009\n",
      "Iteration 508, loss = 0.41762626\n",
      "Iteration 509, loss = 0.41746284\n",
      "Iteration 510, loss = 0.41730009\n",
      "Iteration 511, loss = 0.41713809\n",
      "Iteration 512, loss = 0.41697662\n",
      "Iteration 513, loss = 0.41681566\n",
      "Iteration 514, loss = 0.41665518\n",
      "Iteration 515, loss = 0.41649517\n",
      "Iteration 516, loss = 0.41633563\n",
      "Iteration 517, loss = 0.41617656\n",
      "Iteration 518, loss = 0.41601796\n",
      "Iteration 519, loss = 0.41585983\n",
      "Iteration 520, loss = 0.41570221\n",
      "Iteration 521, loss = 0.41554514\n",
      "Iteration 522, loss = 0.41538857\n",
      "Iteration 523, loss = 0.41523247\n",
      "Iteration 524, loss = 0.41507682\n",
      "Iteration 525, loss = 0.41492163\n",
      "Iteration 526, loss = 0.41476690\n",
      "Iteration 527, loss = 0.41461261\n",
      "Iteration 528, loss = 0.41445877\n",
      "Iteration 529, loss = 0.41430538\n",
      "Iteration 530, loss = 0.41415243\n",
      "Iteration 531, loss = 0.41399992\n",
      "Iteration 532, loss = 0.41384784\n",
      "Iteration 533, loss = 0.41369620\n",
      "Iteration 534, loss = 0.41354500\n",
      "Iteration 535, loss = 0.41339417\n",
      "Iteration 536, loss = 0.41324367\n",
      "Iteration 537, loss = 0.41309359\n",
      "Iteration 538, loss = 0.41294393\n",
      "Iteration 539, loss = 0.41279468\n",
      "Iteration 540, loss = 0.41264586\n",
      "Iteration 541, loss = 0.41249752\n",
      "Iteration 542, loss = 0.41234960\n",
      "Iteration 543, loss = 0.41220209\n",
      "Iteration 544, loss = 0.41205499\n",
      "Iteration 545, loss = 0.41190829\n",
      "Iteration 546, loss = 0.41176201\n",
      "Iteration 547, loss = 0.41161613\n",
      "Iteration 548, loss = 0.41147066\n",
      "Iteration 549, loss = 0.41132559\n",
      "Iteration 550, loss = 0.41118101\n",
      "Iteration 551, loss = 0.41103698\n",
      "Iteration 552, loss = 0.41089337\n",
      "Iteration 553, loss = 0.41075017\n",
      "Iteration 554, loss = 0.41060737\n",
      "Iteration 555, loss = 0.41046498\n",
      "Iteration 556, loss = 0.41032299\n",
      "Iteration 557, loss = 0.41018140\n",
      "Iteration 558, loss = 0.41004057\n",
      "Iteration 559, loss = 0.40990027\n",
      "Iteration 560, loss = 0.40976040\n",
      "Iteration 561, loss = 0.40962094\n",
      "Iteration 562, loss = 0.40948189\n",
      "Iteration 563, loss = 0.40934335\n",
      "Iteration 564, loss = 0.40920520\n",
      "Iteration 565, loss = 0.40906746\n",
      "Iteration 566, loss = 0.40893021\n",
      "Iteration 567, loss = 0.40879338\n",
      "Iteration 568, loss = 0.40865695\n",
      "Iteration 569, loss = 0.40852091\n",
      "Iteration 570, loss = 0.40838526\n",
      "Iteration 571, loss = 0.40825001\n",
      "Iteration 572, loss = 0.40811520\n",
      "Iteration 573, loss = 0.40798091\n",
      "Iteration 574, loss = 0.40784715\n",
      "Iteration 575, loss = 0.40771377\n",
      "Iteration 576, loss = 0.40758078\n",
      "Iteration 577, loss = 0.40744817\n",
      "Iteration 578, loss = 0.40731593\n",
      "Iteration 579, loss = 0.40718408\n",
      "Iteration 580, loss = 0.40705259\n",
      "Iteration 581, loss = 0.40692175\n",
      "Iteration 582, loss = 0.40679187\n",
      "Iteration 583, loss = 0.40666238\n",
      "Iteration 584, loss = 0.40653337\n",
      "Iteration 585, loss = 0.40640493\n",
      "Iteration 586, loss = 0.40627699\n",
      "Iteration 587, loss = 0.40614946\n",
      "Iteration 588, loss = 0.40602233\n",
      "Iteration 589, loss = 0.40589558\n",
      "Iteration 590, loss = 0.40576921\n",
      "Iteration 591, loss = 0.40564321\n",
      "Iteration 592, loss = 0.40551758\n",
      "Iteration 593, loss = 0.40539229\n",
      "Iteration 594, loss = 0.40526734\n",
      "Iteration 595, loss = 0.40514275\n",
      "Iteration 596, loss = 0.40501857\n",
      "Iteration 597, loss = 0.40489476\n",
      "Iteration 598, loss = 0.40477129\n",
      "Iteration 599, loss = 0.40464816\n",
      "Iteration 600, loss = 0.40452537\n",
      "Iteration 601, loss = 0.40440293\n",
      "Iteration 602, loss = 0.40428085\n",
      "Iteration 603, loss = 0.40415912\n",
      "Iteration 604, loss = 0.40403811\n",
      "Iteration 605, loss = 0.40391778\n",
      "Iteration 606, loss = 0.40379836\n",
      "Iteration 607, loss = 0.40367934\n",
      "Iteration 608, loss = 0.40356077\n",
      "Iteration 609, loss = 0.40344272\n",
      "Iteration 610, loss = 0.40332503\n",
      "Iteration 611, loss = 0.40320771\n",
      "Iteration 612, loss = 0.40309074\n",
      "Iteration 613, loss = 0.40297412\n",
      "Iteration 614, loss = 0.40285784\n",
      "Iteration 615, loss = 0.40274190\n",
      "Iteration 616, loss = 0.40262629\n",
      "Iteration 617, loss = 0.40251101\n",
      "Iteration 618, loss = 0.40239600\n",
      "Iteration 619, loss = 0.40228120\n",
      "Iteration 620, loss = 0.40216677\n",
      "Iteration 621, loss = 0.40205264\n",
      "Iteration 622, loss = 0.40193882\n",
      "Iteration 623, loss = 0.40182531\n",
      "Iteration 624, loss = 0.40171209\n",
      "Iteration 625, loss = 0.40159919\n",
      "Iteration 626, loss = 0.40148669\n",
      "Iteration 627, loss = 0.40137452\n",
      "Iteration 628, loss = 0.40126264\n",
      "Iteration 629, loss = 0.40115106\n",
      "Iteration 630, loss = 0.40103978\n",
      "Iteration 631, loss = 0.40092878\n",
      "Iteration 632, loss = 0.40081808\n",
      "Iteration 633, loss = 0.40070767\n",
      "Iteration 634, loss = 0.40059754\n",
      "Iteration 635, loss = 0.40048769\n",
      "Iteration 636, loss = 0.40037815\n",
      "Iteration 637, loss = 0.40026911\n",
      "Iteration 638, loss = 0.40016038\n",
      "Iteration 639, loss = 0.40005194\n",
      "Iteration 640, loss = 0.39994378\n",
      "Iteration 641, loss = 0.39983592\n",
      "Iteration 642, loss = 0.39972833\n",
      "Iteration 643, loss = 0.39962102\n",
      "Iteration 644, loss = 0.39951399\n",
      "Iteration 645, loss = 0.39940724\n",
      "Iteration 646, loss = 0.39930076\n",
      "Iteration 647, loss = 0.39919456\n",
      "Iteration 648, loss = 0.39908862\n",
      "Iteration 649, loss = 0.39898296\n",
      "Iteration 650, loss = 0.39887764\n",
      "Iteration 651, loss = 0.39877266\n",
      "Iteration 652, loss = 0.39866794\n",
      "Iteration 653, loss = 0.39856352\n",
      "Iteration 654, loss = 0.39845933\n",
      "Iteration 655, loss = 0.39835382\n",
      "Iteration 656, loss = 0.39824786\n",
      "Iteration 657, loss = 0.39814190\n",
      "Iteration 658, loss = 0.39803594\n",
      "Iteration 659, loss = 0.39793001\n",
      "Iteration 660, loss = 0.39782414\n",
      "Iteration 661, loss = 0.39771833\n",
      "Iteration 662, loss = 0.39761261\n",
      "Iteration 663, loss = 0.39750699\n",
      "Iteration 664, loss = 0.39740149\n",
      "Iteration 665, loss = 0.39729612\n",
      "Iteration 666, loss = 0.39719097\n",
      "Iteration 667, loss = 0.39708595\n",
      "Iteration 668, loss = 0.39698107\n",
      "Iteration 669, loss = 0.39687634\n",
      "Iteration 670, loss = 0.39677176\n",
      "Iteration 671, loss = 0.39666737\n",
      "Iteration 672, loss = 0.39656318\n",
      "Iteration 673, loss = 0.39645918\n",
      "Iteration 674, loss = 0.39635537\n",
      "Iteration 675, loss = 0.39625177\n",
      "Iteration 676, loss = 0.39614833\n",
      "Iteration 677, loss = 0.39604508\n",
      "Iteration 678, loss = 0.39594203\n",
      "Iteration 679, loss = 0.39583919\n",
      "Iteration 680, loss = 0.39573656\n",
      "Iteration 681, loss = 0.39563414\n",
      "Iteration 682, loss = 0.39553184\n",
      "Iteration 683, loss = 0.39542966\n",
      "Iteration 684, loss = 0.39532769\n",
      "Iteration 685, loss = 0.39522591\n",
      "Iteration 686, loss = 0.39512434\n",
      "Iteration 687, loss = 0.39502298\n",
      "Iteration 688, loss = 0.39492171\n",
      "Iteration 689, loss = 0.39482050\n",
      "Iteration 690, loss = 0.39471991\n",
      "Iteration 691, loss = 0.39461998\n",
      "Iteration 692, loss = 0.39452033\n",
      "Iteration 693, loss = 0.39442100\n",
      "Iteration 694, loss = 0.39432193\n",
      "Iteration 695, loss = 0.39422311\n",
      "Iteration 696, loss = 0.39412454\n",
      "Iteration 697, loss = 0.39402621\n",
      "Iteration 698, loss = 0.39392817\n",
      "Iteration 699, loss = 0.39383039\n",
      "Iteration 700, loss = 0.39373286\n",
      "Iteration 701, loss = 0.39363556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42) #, stratify=y\n",
    "red = MLPClassifier(hidden_layer_sizes=(50,), \n",
    "                    solver='sgd', \n",
    "#                     learning_rate_init=0.0001, \n",
    "#                     activation='relu', \n",
    "                    random_state=0, \n",
    "                    verbose=True, \n",
    "                    max_iter=1000)\n",
    "history = red.fit(X_train, y_train) \n",
    "\n",
    "print(len(X_train))\n",
    "# lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89715307,  0.94175457],\n",
       "       [ 1.06821751,  0.75846569],\n",
       "       [-0.4993884 ,  0.13192906],\n",
       "       [-0.46333991,  0.86330772],\n",
       "       [-0.7280717 ,  0.3259131 ],\n",
       "       [ 1.72532644,  0.53367598],\n",
       "       [ 2.11889248,  0.60498388],\n",
       "       [ 1.31702684, -0.2525239 ],\n",
       "       [ 1.11634545,  0.01823342],\n",
       "       [ 0.55039452,  1.16554689],\n",
       "       [ 0.36877983, -0.34894509],\n",
       "       [ 0.42598043, -0.3006242 ],\n",
       "       [-0.24608615,  0.378107  ],\n",
       "       [-0.60690411,  0.50000529],\n",
       "       [ 1.15536561, -0.50593577],\n",
       "       [ 1.26285558,  0.12916271],\n",
       "       [-0.59385445,  0.46769065],\n",
       "       [ 1.12856036,  0.33191968],\n",
       "       [-0.04686928, -0.01567029],\n",
       "       [ 0.8729088 ,  0.08643291],\n",
       "       [ 0.01856462,  1.32827802],\n",
       "       [ 1.00549331,  0.38686701],\n",
       "       [-0.16955317,  0.60660877],\n",
       "       [ 1.50917461, -0.06701048],\n",
       "       [-0.38099245,  1.34740194],\n",
       "       [-0.51699811,  0.74457804],\n",
       "       [ 1.31311917, -0.69665985],\n",
       "       [ 0.77145295, -0.69709227],\n",
       "       [ 0.97370054, -0.08631168],\n",
       "       [-0.74872343, -0.06972957]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La precisión sobre el dataset de entrenamiento es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285714285714286"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = red.score(X_train, y_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La precisión sobre el dataset de prueba es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = red.score(X_test, y_test)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_2d_separator(red, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train) \n",
    "plt.xlabel(\"Feature 0\") \n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicio\n",
    "Resuelva el problema del XOR con 2 neuronas en la capa oculta. Considere utilizar una función de activación apropiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "Y = np.array([[-1], [1], [1],[-1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pruebe el código utilizando:\n",
    "1. SGD\n",
    "2. lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "red_xor = MLPClassifier(hidden_layer_sizes=(4),\n",
    "                        activation='tanh',\n",
    "                        solver='lbfgs',\n",
    "                        max_iter=1500, verbose=1)\n",
    "#lbfgs / lbfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Entrenando la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=4, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_xor.fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_xor.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, -1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = red_xor.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_grafico = [-1, 1, 1, -1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1], c=y_grafico, cmap=cm_bright)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mglearn \n",
    "mglearn.plots.plot_2d_separator(red_xor, X, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y_grafico) \n",
    "plt.xlabel(\"Feature 0\") \n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejercicios adicionales\n",
    "Resuelva con una arquitectura de red neuronal feed forward multicapa el dataset iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Red Neuronal como Regresión\n",
    "Considere una función $y = x^2-2x+3$. Para el intervalo de $[-5, 5]$ Obtenga 1000 puntos. Puede hacer que la red neuronal reproduzca esta función en el Intervalo propuesto?\n",
    "\n",
    "Que pasa si incrementa el dataset a 2000 muestras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5, num=2000)\n",
    "# x = np.arange(10)\n",
    "y = x*x-2*x+3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.       , -4.9949975, -4.989995 , ...,  4.989995 ,  4.9949975,\n",
       "        5.       ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = x.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 101.93303233\n",
      "Iteration 2, loss = 90.76067721\n",
      "Iteration 3, loss = 79.85949182\n",
      "Iteration 4, loss = 69.15850697\n",
      "Iteration 5, loss = 58.55892530\n",
      "Iteration 6, loss = 47.90123440\n",
      "Iteration 7, loss = 37.84297301\n",
      "Iteration 8, loss = 28.71243352\n",
      "Iteration 9, loss = 20.82862983\n",
      "Iteration 10, loss = 14.56542971\n",
      "Iteration 11, loss = 9.84558175\n",
      "Iteration 12, loss = 6.76176695\n",
      "Iteration 13, loss = 4.87979779\n",
      "Iteration 14, loss = 3.96877321\n",
      "Iteration 15, loss = 3.54963555\n",
      "Iteration 16, loss = 3.36912210\n",
      "Iteration 17, loss = 3.26834599\n",
      "Iteration 18, loss = 3.18340166\n",
      "Iteration 19, loss = 3.10396540\n",
      "Iteration 20, loss = 3.02705360\n",
      "Iteration 21, loss = 2.95397777\n",
      "Iteration 22, loss = 2.88447985\n",
      "Iteration 23, loss = 2.81764871\n",
      "Iteration 24, loss = 2.75451414\n",
      "Iteration 25, loss = 2.69295531\n",
      "Iteration 26, loss = 2.63396540\n",
      "Iteration 27, loss = 2.57774380\n",
      "Iteration 28, loss = 2.52274117\n",
      "Iteration 29, loss = 2.46867603\n",
      "Iteration 30, loss = 2.41722731\n",
      "Iteration 31, loss = 2.36764837\n",
      "Iteration 32, loss = 2.31892663\n",
      "Iteration 33, loss = 2.27272235\n",
      "Iteration 34, loss = 2.22688383\n",
      "Iteration 35, loss = 2.18236764\n",
      "Iteration 36, loss = 2.13993752\n",
      "Iteration 37, loss = 2.09707002\n",
      "Iteration 38, loss = 2.05625054\n",
      "Iteration 39, loss = 2.01689274\n",
      "Iteration 40, loss = 1.97796411\n",
      "Iteration 41, loss = 1.93903683\n",
      "Iteration 42, loss = 1.90201327\n",
      "Iteration 43, loss = 1.86658151\n",
      "Iteration 44, loss = 1.83004011\n",
      "Iteration 45, loss = 1.79676090\n",
      "Iteration 46, loss = 1.76111769\n",
      "Iteration 47, loss = 1.72930266\n",
      "Iteration 48, loss = 1.69682670\n",
      "Iteration 49, loss = 1.66427449\n",
      "Iteration 50, loss = 1.63408693\n",
      "Iteration 51, loss = 1.60380824\n",
      "Iteration 52, loss = 1.57481917\n",
      "Iteration 53, loss = 1.54596919\n",
      "Iteration 54, loss = 1.51774485\n",
      "Iteration 55, loss = 1.49073889\n",
      "Iteration 56, loss = 1.46375153\n",
      "Iteration 57, loss = 1.43683855\n",
      "Iteration 58, loss = 1.41155799\n",
      "Iteration 59, loss = 1.38618825\n",
      "Iteration 60, loss = 1.36060710\n",
      "Iteration 61, loss = 1.33634913\n",
      "Iteration 62, loss = 1.31261796\n",
      "Iteration 63, loss = 1.28941291\n",
      "Iteration 64, loss = 1.26631204\n",
      "Iteration 65, loss = 1.24542545\n",
      "Iteration 66, loss = 1.22199318\n",
      "Iteration 67, loss = 1.20059968\n",
      "Iteration 68, loss = 1.17726363\n",
      "Iteration 69, loss = 1.15678553\n",
      "Iteration 70, loss = 1.13710398\n",
      "Iteration 71, loss = 1.11515421\n",
      "Iteration 72, loss = 1.09352635\n",
      "Iteration 73, loss = 1.07333836\n",
      "Iteration 74, loss = 1.05361934\n",
      "Iteration 75, loss = 1.03387747\n",
      "Iteration 76, loss = 1.01454197\n",
      "Iteration 77, loss = 0.99546776\n",
      "Iteration 78, loss = 0.97472211\n",
      "Iteration 79, loss = 0.95664479\n",
      "Iteration 80, loss = 0.93728140\n",
      "Iteration 81, loss = 0.91833524\n",
      "Iteration 82, loss = 0.89913539\n",
      "Iteration 83, loss = 0.88157089\n",
      "Iteration 84, loss = 0.86225400\n",
      "Iteration 85, loss = 0.84456621\n",
      "Iteration 86, loss = 0.82680496\n",
      "Iteration 87, loss = 0.80866465\n",
      "Iteration 88, loss = 0.79137321\n",
      "Iteration 89, loss = 0.77317695\n",
      "Iteration 90, loss = 0.75610855\n",
      "Iteration 91, loss = 0.73871131\n",
      "Iteration 92, loss = 0.72201592\n",
      "Iteration 93, loss = 0.70488646\n",
      "Iteration 94, loss = 0.68856832\n",
      "Iteration 95, loss = 0.67294138\n",
      "Iteration 96, loss = 0.65601560\n",
      "Iteration 97, loss = 0.64031743\n",
      "Iteration 98, loss = 0.62382383\n",
      "Iteration 99, loss = 0.60894270\n",
      "Iteration 100, loss = 0.59363404\n",
      "Iteration 101, loss = 0.57818969\n",
      "Iteration 102, loss = 0.56353424\n",
      "Iteration 103, loss = 0.54911631\n",
      "Iteration 104, loss = 0.53484603\n",
      "Iteration 105, loss = 0.52111346\n",
      "Iteration 106, loss = 0.50749952\n",
      "Iteration 107, loss = 0.49405009\n",
      "Iteration 108, loss = 0.48064318\n",
      "Iteration 109, loss = 0.46797491\n",
      "Iteration 110, loss = 0.45586135\n",
      "Iteration 111, loss = 0.44267825\n",
      "Iteration 112, loss = 0.43067213\n",
      "Iteration 113, loss = 0.41912231\n",
      "Iteration 114, loss = 0.40725133\n",
      "Iteration 115, loss = 0.39653945\n",
      "Iteration 116, loss = 0.38562524\n",
      "Iteration 117, loss = 0.37538115\n",
      "Iteration 118, loss = 0.36474621\n",
      "Iteration 119, loss = 0.35451828\n",
      "Iteration 120, loss = 0.34504730\n",
      "Iteration 121, loss = 0.33508457\n",
      "Iteration 122, loss = 0.32591353\n",
      "Iteration 123, loss = 0.31698928\n",
      "Iteration 124, loss = 0.30813681\n",
      "Iteration 125, loss = 0.29979236\n",
      "Iteration 126, loss = 0.29137641\n",
      "Iteration 127, loss = 0.28298608\n",
      "Iteration 128, loss = 0.27568352\n",
      "Iteration 129, loss = 0.26851483\n",
      "Iteration 130, loss = 0.26013844\n",
      "Iteration 131, loss = 0.25291505\n",
      "Iteration 132, loss = 0.24594719\n",
      "Iteration 133, loss = 0.23932643\n",
      "Iteration 134, loss = 0.23280905\n",
      "Iteration 135, loss = 0.22652140\n",
      "Iteration 136, loss = 0.22059583\n",
      "Iteration 137, loss = 0.21392846\n",
      "Iteration 138, loss = 0.20821961\n",
      "Iteration 139, loss = 0.20242616\n",
      "Iteration 140, loss = 0.19712901\n",
      "Iteration 141, loss = 0.19195775\n",
      "Iteration 142, loss = 0.18641107\n",
      "Iteration 143, loss = 0.18177072\n",
      "Iteration 144, loss = 0.17675933\n",
      "Iteration 145, loss = 0.17218695\n",
      "Iteration 146, loss = 0.16704828\n",
      "Iteration 147, loss = 0.16264197\n",
      "Iteration 148, loss = 0.15896711\n",
      "Iteration 149, loss = 0.15456174\n",
      "Iteration 150, loss = 0.15016797\n",
      "Iteration 151, loss = 0.14611771\n",
      "Iteration 152, loss = 0.14203329\n",
      "Iteration 153, loss = 0.13818320\n",
      "Iteration 154, loss = 0.13465811\n",
      "Iteration 155, loss = 0.13094701\n",
      "Iteration 156, loss = 0.12786458\n",
      "Iteration 157, loss = 0.12448937\n",
      "Iteration 158, loss = 0.12080260\n",
      "Iteration 159, loss = 0.11759807\n",
      "Iteration 160, loss = 0.11511990\n",
      "Iteration 161, loss = 0.11199846\n",
      "Iteration 162, loss = 0.10851635\n",
      "Iteration 163, loss = 0.10612695\n",
      "Iteration 164, loss = 0.10339752\n",
      "Iteration 165, loss = 0.10025105\n",
      "Iteration 166, loss = 0.09798898\n",
      "Iteration 167, loss = 0.09539400\n",
      "Iteration 168, loss = 0.09296035\n",
      "Iteration 169, loss = 0.09071031\n",
      "Iteration 170, loss = 0.08808518\n",
      "Iteration 171, loss = 0.08589156\n",
      "Iteration 172, loss = 0.08380856\n",
      "Iteration 173, loss = 0.08150688\n",
      "Iteration 174, loss = 0.07991928\n",
      "Iteration 175, loss = 0.07767277\n",
      "Iteration 176, loss = 0.07572810\n",
      "Iteration 177, loss = 0.07375791\n",
      "Iteration 178, loss = 0.07195046\n",
      "Iteration 179, loss = 0.07014485\n",
      "Iteration 180, loss = 0.06853586\n",
      "Iteration 181, loss = 0.06675036\n",
      "Iteration 182, loss = 0.06519459\n",
      "Iteration 183, loss = 0.06355229\n",
      "Iteration 184, loss = 0.06198923\n",
      "Iteration 185, loss = 0.06053462\n",
      "Iteration 186, loss = 0.05906474\n",
      "Iteration 187, loss = 0.05781891\n",
      "Iteration 188, loss = 0.05644163\n",
      "Iteration 189, loss = 0.05509340\n",
      "Iteration 190, loss = 0.05374996\n",
      "Iteration 191, loss = 0.05260195\n",
      "Iteration 192, loss = 0.05154501\n",
      "Iteration 193, loss = 0.05037206\n",
      "Iteration 194, loss = 0.04927386\n",
      "Iteration 195, loss = 0.04800311\n",
      "Iteration 196, loss = 0.04696457\n",
      "Iteration 197, loss = 0.04601408\n",
      "Iteration 198, loss = 0.04481177\n",
      "Iteration 199, loss = 0.04395238\n",
      "Iteration 200, loss = 0.04342041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "red_funcion = MLPRegressor(hidden_layer_sizes=(500,), solver='adam', activation='relu', verbose=1)\n",
    "red_funcion.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990727685870955"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_funcion.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(-5,5,500)\n",
    "y_real = x_test*x_test-2*x_test+3\n",
    "x_test = x_test.reshape(-1,1)\n",
    "# x_test\n",
    "y_pred = red_funcion.predict(x_test)\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_test, y_real, color='blue')\n",
    "plt.plot(x_test, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Qué sucede si deseo calcular para un valor fuera del rango como por ejemplo $x=5.5$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num = 16.\n",
    "y_num = x_num*x_num-2*x_num+3\n",
    "y_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82.10280574])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num = np.asarray(x_num)\n",
    "x_num = x_num.reshape(-1,1)\n",
    "y_num_pred = red_funcion.predict(x_num)\n",
    "y_num_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resolviendo el problema de clasificación del iris dataset\n",
    "Considere el problema de clasificar las flores iris según la longitud y ancho del sépalo y el pétolo. Resuelva este problema utilizando Redes Neuronales Multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "print(iris_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los primeros 5 datos del dataset son $\\mathbf{\\mathcal{X}}:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset['data'][0:5,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las etiquetas para las **muestras** o **samples** anteriores son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset['target'][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Paso los datos del iris dataset a una variable $\\mathbf{\\mathcal{X}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = iris_dataset['data']\n",
    "Y = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Codificando las clases como **ONE HOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_one_hot = np_utils.to_categorical(Y)\n",
    "Y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Escalando los valores de $\\mathbf{\\mathcal{X}}$. Sin embargo primero pruebe el entrenamiento sin escalar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La media [5.84333333 3.05733333 3.758      1.19933333]\n",
      "Escalado de X, se muestra primeras 5 filas \n",
      "[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n",
      "min -2.43394714190809\n",
      "max 3.0907752482994253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "funcion_escalado = scaler.fit(X)\n",
    "# X_escalado = X\n",
    "X_escalado = funcion_escalado.transform(X)\n",
    "print('La media {}'.format(scaler.mean_))\n",
    "print('Escalado de X, se muestra primeras 5 filas \\n{}'.format(X_escalado[0:5,:]))\n",
    "print('min {}'.format(X_escalado.min()))\n",
    "print('max {}'.format(X_escalado.max()))\n",
    "# StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dividiendo el dataset en train y test. Los datos se deben aleatorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31099753, -0.36217625,  0.53540856,  0.26414192],\n",
       "       [-0.41600969, -1.28296331,  0.13754657,  0.13250973],\n",
       "       [-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
       "       [-1.02184904,  0.78880759, -1.2833891 , -1.3154443 ],\n",
       "       [ 0.31099753, -0.13197948,  0.64908342,  0.79067065]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_escalado, Y_one_hot, test_size=0.2, random_state=73)\n",
    "X_train[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creando un Modelo de Redes Neuronales Artificiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "red_iris = MLPClassifier(hidden_layer_sizes=(3,),\n",
    "                        activation='tanh',\n",
    "                        solver='sgd',\n",
    "                        momentum=0.95,\n",
    "                        max_iter=1500, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ajuste de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.40141643\n",
      "Iteration 2, loss = 2.40012553\n",
      "Iteration 3, loss = 2.39823968\n",
      "Iteration 4, loss = 2.39579135\n",
      "Iteration 5, loss = 2.39281200\n",
      "Iteration 6, loss = 2.38933216\n",
      "Iteration 7, loss = 2.38538133\n",
      "Iteration 8, loss = 2.38098801\n",
      "Iteration 9, loss = 2.37617971\n",
      "Iteration 10, loss = 2.37098292\n",
      "Iteration 11, loss = 2.36542317\n",
      "Iteration 12, loss = 2.35952495\n",
      "Iteration 13, loss = 2.35331183\n",
      "Iteration 14, loss = 2.34680638\n",
      "Iteration 15, loss = 2.34003026\n",
      "Iteration 16, loss = 2.33300420\n",
      "Iteration 17, loss = 2.32574802\n",
      "Iteration 18, loss = 2.31828069\n",
      "Iteration 19, loss = 2.31062028\n",
      "Iteration 20, loss = 2.30278407\n",
      "Iteration 21, loss = 2.29478851\n",
      "Iteration 22, loss = 2.28664926\n",
      "Iteration 23, loss = 2.27838125\n",
      "Iteration 24, loss = 2.26999864\n",
      "Iteration 25, loss = 2.26151490\n",
      "Iteration 26, loss = 2.25294280\n",
      "Iteration 27, loss = 2.24429447\n",
      "Iteration 28, loss = 2.23558137\n",
      "Iteration 29, loss = 2.22681437\n",
      "Iteration 30, loss = 2.21800373\n",
      "Iteration 31, loss = 2.20915917\n",
      "Iteration 32, loss = 2.20028982\n",
      "Iteration 33, loss = 2.19140431\n",
      "Iteration 34, loss = 2.18251077\n",
      "Iteration 35, loss = 2.17361684\n",
      "Iteration 36, loss = 2.16472969\n",
      "Iteration 37, loss = 2.15585604\n",
      "Iteration 38, loss = 2.14700221\n",
      "Iteration 39, loss = 2.13817409\n",
      "Iteration 40, loss = 2.12937719\n",
      "Iteration 41, loss = 2.12061664\n",
      "Iteration 42, loss = 2.11189722\n",
      "Iteration 43, loss = 2.10322338\n",
      "Iteration 44, loss = 2.09459923\n",
      "Iteration 45, loss = 2.08602858\n",
      "Iteration 46, loss = 2.07751494\n",
      "Iteration 47, loss = 2.06906154\n",
      "Iteration 48, loss = 2.06067134\n",
      "Iteration 49, loss = 2.05234706\n",
      "Iteration 50, loss = 2.04409116\n",
      "Iteration 51, loss = 2.03590588\n",
      "Iteration 52, loss = 2.02779324\n",
      "Iteration 53, loss = 2.01975505\n",
      "Iteration 54, loss = 2.01179292\n",
      "Iteration 55, loss = 2.00390828\n",
      "Iteration 56, loss = 1.99610238\n",
      "Iteration 57, loss = 1.98837629\n",
      "Iteration 58, loss = 1.98073095\n",
      "Iteration 59, loss = 1.97316712\n",
      "Iteration 60, loss = 1.96568543\n",
      "Iteration 61, loss = 1.95828639\n",
      "Iteration 62, loss = 1.95097035\n",
      "Iteration 63, loss = 1.94373758\n",
      "Iteration 64, loss = 1.93658821\n",
      "Iteration 65, loss = 1.92952227\n",
      "Iteration 66, loss = 1.92253970\n",
      "Iteration 67, loss = 1.91564034\n",
      "Iteration 68, loss = 1.90882395\n",
      "Iteration 69, loss = 1.90209019\n",
      "Iteration 70, loss = 1.89543867\n",
      "Iteration 71, loss = 1.88886892\n",
      "Iteration 72, loss = 1.88238039\n",
      "Iteration 73, loss = 1.87597248\n",
      "Iteration 74, loss = 1.86964454\n",
      "Iteration 75, loss = 1.86339587\n",
      "Iteration 76, loss = 1.85722569\n",
      "Iteration 77, loss = 1.85113321\n",
      "Iteration 78, loss = 1.84511760\n",
      "Iteration 79, loss = 1.83917796\n",
      "Iteration 80, loss = 1.83331339\n",
      "Iteration 81, loss = 1.82752295\n",
      "Iteration 82, loss = 1.82180566\n",
      "Iteration 83, loss = 1.81616054\n",
      "Iteration 84, loss = 1.81058655\n",
      "Iteration 85, loss = 1.80508268\n",
      "Iteration 86, loss = 1.79964786\n",
      "Iteration 87, loss = 1.79428104\n",
      "Iteration 88, loss = 1.78898113\n",
      "Iteration 89, loss = 1.78374705\n",
      "Iteration 90, loss = 1.77857770\n",
      "Iteration 91, loss = 1.77347199\n",
      "Iteration 92, loss = 1.76842880\n",
      "Iteration 93, loss = 1.76344704\n",
      "Iteration 94, loss = 1.75852559\n",
      "Iteration 95, loss = 1.75366335\n",
      "Iteration 96, loss = 1.74885922\n",
      "Iteration 97, loss = 1.74411209\n",
      "Iteration 98, loss = 1.73942087\n",
      "Iteration 99, loss = 1.73478446\n",
      "Iteration 100, loss = 1.73020179\n",
      "Iteration 101, loss = 1.72567178\n",
      "Iteration 102, loss = 1.72119337\n",
      "Iteration 103, loss = 1.71676549\n",
      "Iteration 104, loss = 1.71238711\n",
      "Iteration 105, loss = 1.70805719\n",
      "Iteration 106, loss = 1.70377470\n",
      "Iteration 107, loss = 1.69953864\n",
      "Iteration 108, loss = 1.69534801\n",
      "Iteration 109, loss = 1.69120184\n",
      "Iteration 110, loss = 1.68709914\n",
      "Iteration 111, loss = 1.68303897\n",
      "Iteration 112, loss = 1.67902038\n",
      "Iteration 113, loss = 1.67504246\n",
      "Iteration 114, loss = 1.67110430\n",
      "Iteration 115, loss = 1.66720500\n",
      "Iteration 116, loss = 1.66334369\n",
      "Iteration 117, loss = 1.65951951\n",
      "Iteration 118, loss = 1.65573161\n",
      "Iteration 119, loss = 1.65197917\n",
      "Iteration 120, loss = 1.64826137\n",
      "Iteration 121, loss = 1.64457742\n",
      "Iteration 122, loss = 1.64092655\n",
      "Iteration 123, loss = 1.63730799\n",
      "Iteration 124, loss = 1.63372099\n",
      "Iteration 125, loss = 1.63016484\n",
      "Iteration 126, loss = 1.62663881\n",
      "Iteration 127, loss = 1.62314221\n",
      "Iteration 128, loss = 1.61967436\n",
      "Iteration 129, loss = 1.61623460\n",
      "Iteration 130, loss = 1.61282228\n",
      "Iteration 131, loss = 1.60943677\n",
      "Iteration 132, loss = 1.60607745\n",
      "Iteration 133, loss = 1.60274372\n",
      "Iteration 134, loss = 1.59943500\n",
      "Iteration 135, loss = 1.59615071\n",
      "Iteration 136, loss = 1.59289030\n",
      "Iteration 137, loss = 1.58965324\n",
      "Iteration 138, loss = 1.58643899\n",
      "Iteration 139, loss = 1.58324703\n",
      "Iteration 140, loss = 1.58007689\n",
      "Iteration 141, loss = 1.57692806\n",
      "Iteration 142, loss = 1.57380007\n",
      "Iteration 143, loss = 1.57069248\n",
      "Iteration 144, loss = 1.56760483\n",
      "Iteration 145, loss = 1.56453670\n",
      "Iteration 146, loss = 1.56148766\n",
      "Iteration 147, loss = 1.55845731\n",
      "Iteration 148, loss = 1.55544524\n",
      "Iteration 149, loss = 1.55245109\n",
      "Iteration 150, loss = 1.54947447\n",
      "Iteration 151, loss = 1.54651502\n",
      "Iteration 152, loss = 1.54357240\n",
      "Iteration 153, loss = 1.54064627\n",
      "Iteration 154, loss = 1.53773629\n",
      "Iteration 155, loss = 1.53484215\n",
      "Iteration 156, loss = 1.53196354\n",
      "Iteration 157, loss = 1.52910016\n",
      "Iteration 158, loss = 1.52625172\n",
      "Iteration 159, loss = 1.52341794\n",
      "Iteration 160, loss = 1.52059855\n",
      "Iteration 161, loss = 1.51779328\n",
      "Iteration 162, loss = 1.51500189\n",
      "Iteration 163, loss = 1.51222412\n",
      "Iteration 164, loss = 1.50945975\n",
      "Iteration 165, loss = 1.50670853\n",
      "Iteration 166, loss = 1.50397024\n",
      "Iteration 167, loss = 1.50124468\n",
      "Iteration 168, loss = 1.49853163\n",
      "Iteration 169, loss = 1.49583089\n",
      "Iteration 170, loss = 1.49314226\n",
      "Iteration 171, loss = 1.49046557\n",
      "Iteration 172, loss = 1.48780062\n",
      "Iteration 173, loss = 1.48514724\n",
      "Iteration 174, loss = 1.48250527\n",
      "Iteration 175, loss = 1.47987453\n",
      "Iteration 176, loss = 1.47725488\n",
      "Iteration 177, loss = 1.47464615\n",
      "Iteration 178, loss = 1.47204820\n",
      "Iteration 179, loss = 1.46946088\n",
      "Iteration 180, loss = 1.46688407\n",
      "Iteration 181, loss = 1.46431762\n",
      "Iteration 182, loss = 1.46176141\n",
      "Iteration 183, loss = 1.45921531\n",
      "Iteration 184, loss = 1.45667921\n",
      "Iteration 185, loss = 1.45415298\n",
      "Iteration 186, loss = 1.45163652\n",
      "Iteration 187, loss = 1.44912971\n",
      "Iteration 188, loss = 1.44663246\n",
      "Iteration 189, loss = 1.44414466\n",
      "Iteration 190, loss = 1.44166622\n",
      "Iteration 191, loss = 1.43919703\n",
      "Iteration 192, loss = 1.43673701\n",
      "Iteration 193, loss = 1.43428608\n",
      "Iteration 194, loss = 1.43184414\n",
      "Iteration 195, loss = 1.42941112\n",
      "Iteration 196, loss = 1.42698693\n",
      "Iteration 197, loss = 1.42457149\n",
      "Iteration 198, loss = 1.42216474\n",
      "Iteration 199, loss = 1.41976660\n",
      "Iteration 200, loss = 1.41737701\n",
      "Iteration 201, loss = 1.41499588\n",
      "Iteration 202, loss = 1.41262316\n",
      "Iteration 203, loss = 1.41025879\n",
      "Iteration 204, loss = 1.40790270\n",
      "Iteration 205, loss = 1.40555483\n",
      "Iteration 206, loss = 1.40321513\n",
      "Iteration 207, loss = 1.40088354\n",
      "Iteration 208, loss = 1.39856000\n",
      "Iteration 209, loss = 1.39624446\n",
      "Iteration 210, loss = 1.39393688\n",
      "Iteration 211, loss = 1.39163719\n",
      "Iteration 212, loss = 1.38934536\n",
      "Iteration 213, loss = 1.38706133\n",
      "Iteration 214, loss = 1.38478506\n",
      "Iteration 215, loss = 1.38251650\n",
      "Iteration 216, loss = 1.38025561\n",
      "Iteration 217, loss = 1.37800235\n",
      "Iteration 218, loss = 1.37575667\n",
      "Iteration 219, loss = 1.37351854\n",
      "Iteration 220, loss = 1.37128791\n",
      "Iteration 221, loss = 1.36906475\n",
      "Iteration 222, loss = 1.36684901\n",
      "Iteration 223, loss = 1.36464067\n",
      "Iteration 224, loss = 1.36243968\n",
      "Iteration 225, loss = 1.36024601\n",
      "Iteration 226, loss = 1.35805962\n",
      "Iteration 227, loss = 1.35588047\n",
      "Iteration 228, loss = 1.35370855\n",
      "Iteration 229, loss = 1.35154380\n",
      "Iteration 230, loss = 1.34938620\n",
      "Iteration 231, loss = 1.34723572\n",
      "Iteration 232, loss = 1.34509233\n",
      "Iteration 233, loss = 1.34295599\n",
      "Iteration 234, loss = 1.34082667\n",
      "Iteration 235, loss = 1.33870434\n",
      "Iteration 236, loss = 1.33658898\n",
      "Iteration 237, loss = 1.33448054\n",
      "Iteration 238, loss = 1.33237902\n",
      "Iteration 239, loss = 1.33028436\n",
      "Iteration 240, loss = 1.32819655\n",
      "Iteration 241, loss = 1.32611556\n",
      "Iteration 242, loss = 1.32404136\n",
      "Iteration 243, loss = 1.32197392\n",
      "Iteration 244, loss = 1.31991322\n",
      "Iteration 245, loss = 1.31785922\n",
      "Iteration 246, loss = 1.31581190\n",
      "Iteration 247, loss = 1.31377123\n",
      "Iteration 248, loss = 1.31173719\n",
      "Iteration 249, loss = 1.30970975\n",
      "Iteration 250, loss = 1.30768888\n",
      "Iteration 251, loss = 1.30567456\n",
      "Iteration 252, loss = 1.30366676\n",
      "Iteration 253, loss = 1.30166545\n",
      "Iteration 254, loss = 1.29967061\n",
      "Iteration 255, loss = 1.29768221\n",
      "Iteration 256, loss = 1.29570023\n",
      "Iteration 257, loss = 1.29372464\n",
      "Iteration 258, loss = 1.29175542\n",
      "Iteration 259, loss = 1.28979253\n",
      "Iteration 260, loss = 1.28783597\n",
      "Iteration 261, loss = 1.28588569\n",
      "Iteration 262, loss = 1.28394167\n",
      "Iteration 263, loss = 1.28200389\n",
      "Iteration 264, loss = 1.28007233\n",
      "Iteration 265, loss = 1.27814696\n",
      "Iteration 266, loss = 1.27622775\n",
      "Iteration 267, loss = 1.27431468\n",
      "Iteration 268, loss = 1.27240772\n",
      "Iteration 269, loss = 1.27050686\n",
      "Iteration 270, loss = 1.26861206\n",
      "Iteration 271, loss = 1.26672329\n",
      "Iteration 272, loss = 1.26484055\n",
      "Iteration 273, loss = 1.26296379\n",
      "Iteration 274, loss = 1.26109300\n",
      "Iteration 275, loss = 1.25922815\n",
      "Iteration 276, loss = 1.25736921\n",
      "Iteration 277, loss = 1.25551617\n",
      "Iteration 278, loss = 1.25366899\n",
      "Iteration 279, loss = 1.25182766\n",
      "Iteration 280, loss = 1.24999214\n",
      "Iteration 281, loss = 1.24816242\n",
      "Iteration 282, loss = 1.24633846\n",
      "Iteration 283, loss = 1.24452025\n",
      "Iteration 284, loss = 1.24270776\n",
      "Iteration 285, loss = 1.24090097\n",
      "Iteration 286, loss = 1.23909984\n",
      "Iteration 287, loss = 1.23730437\n",
      "Iteration 288, loss = 1.23551451\n",
      "Iteration 289, loss = 1.23373025\n",
      "Iteration 290, loss = 1.23195157\n",
      "Iteration 291, loss = 1.23017843\n",
      "Iteration 292, loss = 1.22841082\n",
      "Iteration 293, loss = 1.22664871\n",
      "Iteration 294, loss = 1.22489208\n",
      "Iteration 295, loss = 1.22314090\n",
      "Iteration 296, loss = 1.22139514\n",
      "Iteration 297, loss = 1.21965479\n",
      "Iteration 298, loss = 1.21791982\n",
      "Iteration 299, loss = 1.21619020\n",
      "Iteration 300, loss = 1.21446592\n",
      "Iteration 301, loss = 1.21274694\n",
      "Iteration 302, loss = 1.21103324\n",
      "Iteration 303, loss = 1.20932481\n",
      "Iteration 304, loss = 1.20762160\n",
      "Iteration 305, loss = 1.20592361\n",
      "Iteration 306, loss = 1.20423081\n",
      "Iteration 307, loss = 1.20254317\n",
      "Iteration 308, loss = 1.20086066\n",
      "Iteration 309, loss = 1.19918327\n",
      "Iteration 310, loss = 1.19751098\n",
      "Iteration 311, loss = 1.19584375\n",
      "Iteration 312, loss = 1.19418156\n",
      "Iteration 313, loss = 1.19252440\n",
      "Iteration 314, loss = 1.19087223\n",
      "Iteration 315, loss = 1.18922503\n",
      "Iteration 316, loss = 1.18758279\n",
      "Iteration 317, loss = 1.18594546\n",
      "Iteration 318, loss = 1.18431304\n",
      "Iteration 319, loss = 1.18268550\n",
      "Iteration 320, loss = 1.18106281\n",
      "Iteration 321, loss = 1.17944496\n",
      "Iteration 322, loss = 1.17783191\n",
      "Iteration 323, loss = 1.17622365\n",
      "Iteration 324, loss = 1.17462015\n",
      "Iteration 325, loss = 1.17302138\n",
      "Iteration 326, loss = 1.17142734\n",
      "Iteration 327, loss = 1.16983798\n",
      "Iteration 328, loss = 1.16825329\n",
      "Iteration 329, loss = 1.16667325\n",
      "Iteration 330, loss = 1.16509783\n",
      "Iteration 331, loss = 1.16352701\n",
      "Iteration 332, loss = 1.16196077\n",
      "Iteration 333, loss = 1.16039908\n",
      "Iteration 334, loss = 1.15884192\n",
      "Iteration 335, loss = 1.15728927\n",
      "Iteration 336, loss = 1.15574110\n",
      "Iteration 337, loss = 1.15419740\n",
      "Iteration 338, loss = 1.15265814\n",
      "Iteration 339, loss = 1.15112329\n",
      "Iteration 340, loss = 1.14959284\n",
      "Iteration 341, loss = 1.14806677\n",
      "Iteration 342, loss = 1.14654504\n",
      "Iteration 343, loss = 1.14502764\n",
      "Iteration 344, loss = 1.14351455\n",
      "Iteration 345, loss = 1.14200574\n",
      "Iteration 346, loss = 1.14050120\n",
      "Iteration 347, loss = 1.13900089\n",
      "Iteration 348, loss = 1.13750480\n",
      "Iteration 349, loss = 1.13601290\n",
      "Iteration 350, loss = 1.13452518\n",
      "Iteration 351, loss = 1.13304161\n",
      "Iteration 352, loss = 1.13156217\n",
      "Iteration 353, loss = 1.13008684\n",
      "Iteration 354, loss = 1.12861559\n",
      "Iteration 355, loss = 1.12714841\n",
      "Iteration 356, loss = 1.12568527\n",
      "Iteration 357, loss = 1.12422616\n",
      "Iteration 358, loss = 1.12277104\n",
      "Iteration 359, loss = 1.12131990\n",
      "Iteration 360, loss = 1.11987273\n",
      "Iteration 361, loss = 1.11842948\n",
      "Iteration 362, loss = 1.11699015\n",
      "Iteration 363, loss = 1.11555472\n",
      "Iteration 364, loss = 1.11412316\n",
      "Iteration 365, loss = 1.11269545\n",
      "Iteration 366, loss = 1.11127158\n",
      "Iteration 367, loss = 1.10985151\n",
      "Iteration 368, loss = 1.10843524\n",
      "Iteration 369, loss = 1.10702273\n",
      "Iteration 370, loss = 1.10561397\n",
      "Iteration 371, loss = 1.10420894\n",
      "Iteration 372, loss = 1.10280762\n",
      "Iteration 373, loss = 1.10140999\n",
      "Iteration 374, loss = 1.10001602\n",
      "Iteration 375, loss = 1.09862570\n",
      "Iteration 376, loss = 1.09723900\n",
      "Iteration 377, loss = 1.09585591\n",
      "Iteration 378, loss = 1.09447641\n",
      "Iteration 379, loss = 1.09310048\n",
      "Iteration 380, loss = 1.09172809\n",
      "Iteration 381, loss = 1.09035922\n",
      "Iteration 382, loss = 1.08899387\n",
      "Iteration 383, loss = 1.08763200\n",
      "Iteration 384, loss = 1.08627360\n",
      "Iteration 385, loss = 1.08491865\n",
      "Iteration 386, loss = 1.08356712\n",
      "Iteration 387, loss = 1.08221901\n",
      "Iteration 388, loss = 1.08087428\n",
      "Iteration 389, loss = 1.07953293\n",
      "Iteration 390, loss = 1.07819492\n",
      "Iteration 391, loss = 1.07686025\n",
      "Iteration 392, loss = 1.07552889\n",
      "Iteration 393, loss = 1.07420083\n",
      "Iteration 394, loss = 1.07287604\n",
      "Iteration 395, loss = 1.07155451\n",
      "Iteration 396, loss = 1.07023621\n",
      "Iteration 397, loss = 1.06892114\n",
      "Iteration 398, loss = 1.06760926\n",
      "Iteration 399, loss = 1.06630057\n",
      "Iteration 400, loss = 1.06499504\n",
      "Iteration 401, loss = 1.06369266\n",
      "Iteration 402, loss = 1.06239340\n",
      "Iteration 403, loss = 1.06109725\n",
      "Iteration 404, loss = 1.05980419\n",
      "Iteration 405, loss = 1.05851421\n",
      "Iteration 406, loss = 1.05722728\n",
      "Iteration 407, loss = 1.05594338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 408, loss = 1.05466250\n",
      "Iteration 409, loss = 1.05338463\n",
      "Iteration 410, loss = 1.05210973\n",
      "Iteration 411, loss = 1.05083780\n",
      "Iteration 412, loss = 1.04956882\n",
      "Iteration 413, loss = 1.04830277\n",
      "Iteration 414, loss = 1.04703963\n",
      "Iteration 415, loss = 1.04577938\n",
      "Iteration 416, loss = 1.04452202\n",
      "Iteration 417, loss = 1.04326751\n",
      "Iteration 418, loss = 1.04201584\n",
      "Iteration 419, loss = 1.04076701\n",
      "Iteration 420, loss = 1.03952098\n",
      "Iteration 421, loss = 1.03827774\n",
      "Iteration 422, loss = 1.03703728\n",
      "Iteration 423, loss = 1.03579957\n",
      "Iteration 424, loss = 1.03456461\n",
      "Iteration 425, loss = 1.03333237\n",
      "Iteration 426, loss = 1.03210284\n",
      "Iteration 427, loss = 1.03087600\n",
      "Iteration 428, loss = 1.02965183\n",
      "Iteration 429, loss = 1.02843032\n",
      "Iteration 430, loss = 1.02721146\n",
      "Iteration 431, loss = 1.02599522\n",
      "Iteration 432, loss = 1.02478158\n",
      "Iteration 433, loss = 1.02357054\n",
      "Iteration 434, loss = 1.02236208\n",
      "Iteration 435, loss = 1.02115618\n",
      "Iteration 436, loss = 1.01995282\n",
      "Iteration 437, loss = 1.01875199\n",
      "Iteration 438, loss = 1.01755367\n",
      "Iteration 439, loss = 1.01635785\n",
      "Iteration 440, loss = 1.01516451\n",
      "Iteration 441, loss = 1.01397364\n",
      "Iteration 442, loss = 1.01278521\n",
      "Iteration 443, loss = 1.01159922\n",
      "Iteration 444, loss = 1.01041565\n",
      "Iteration 445, loss = 1.00923448\n",
      "Iteration 446, loss = 1.00805569\n",
      "Iteration 447, loss = 1.00687928\n",
      "Iteration 448, loss = 1.00570523\n",
      "Iteration 449, loss = 1.00453351\n",
      "Iteration 450, loss = 1.00336413\n",
      "Iteration 451, loss = 1.00219705\n",
      "Iteration 452, loss = 1.00103227\n",
      "Iteration 453, loss = 0.99986977\n",
      "Iteration 454, loss = 0.99870954\n",
      "Iteration 455, loss = 0.99755155\n",
      "Iteration 456, loss = 0.99639581\n",
      "Iteration 457, loss = 0.99524228\n",
      "Iteration 458, loss = 0.99409096\n",
      "Iteration 459, loss = 0.99294184\n",
      "Iteration 460, loss = 0.99179489\n",
      "Iteration 461, loss = 0.99065010\n",
      "Iteration 462, loss = 0.98950746\n",
      "Iteration 463, loss = 0.98836696\n",
      "Iteration 464, loss = 0.98722857\n",
      "Iteration 465, loss = 0.98609229\n",
      "Iteration 466, loss = 0.98495810\n",
      "Iteration 467, loss = 0.98382599\n",
      "Iteration 468, loss = 0.98269594\n",
      "Iteration 469, loss = 0.98156794\n",
      "Iteration 470, loss = 0.98044197\n",
      "Iteration 471, loss = 0.97931803\n",
      "Iteration 472, loss = 0.97819609\n",
      "Iteration 473, loss = 0.97707614\n",
      "Iteration 474, loss = 0.97595817\n",
      "Iteration 475, loss = 0.97484217\n",
      "Iteration 476, loss = 0.97372812\n",
      "Iteration 477, loss = 0.97261600\n",
      "Iteration 478, loss = 0.97150581\n",
      "Iteration 479, loss = 0.97039754\n",
      "Iteration 480, loss = 0.96929115\n",
      "Iteration 481, loss = 0.96818666\n",
      "Iteration 482, loss = 0.96708403\n",
      "Iteration 483, loss = 0.96598326\n",
      "Iteration 484, loss = 0.96488434\n",
      "Iteration 485, loss = 0.96378724\n",
      "Iteration 486, loss = 0.96269197\n",
      "Iteration 487, loss = 0.96159850\n",
      "Iteration 488, loss = 0.96050682\n",
      "Iteration 489, loss = 0.95941692\n",
      "Iteration 490, loss = 0.95832879\n",
      "Iteration 491, loss = 0.95724241\n",
      "Iteration 492, loss = 0.95615777\n",
      "Iteration 493, loss = 0.95507487\n",
      "Iteration 494, loss = 0.95399367\n",
      "Iteration 495, loss = 0.95291418\n",
      "Iteration 496, loss = 0.95183638\n",
      "Iteration 497, loss = 0.95076026\n",
      "Iteration 498, loss = 0.94968581\n",
      "Iteration 499, loss = 0.94861301\n",
      "Iteration 500, loss = 0.94754186\n",
      "Iteration 501, loss = 0.94647233\n",
      "Iteration 502, loss = 0.94540442\n",
      "Iteration 503, loss = 0.94433812\n",
      "Iteration 504, loss = 0.94327341\n",
      "Iteration 505, loss = 0.94221028\n",
      "Iteration 506, loss = 0.94114872\n",
      "Iteration 507, loss = 0.94008873\n",
      "Iteration 508, loss = 0.93903028\n",
      "Iteration 509, loss = 0.93797336\n",
      "Iteration 510, loss = 0.93691797\n",
      "Iteration 511, loss = 0.93586410\n",
      "Iteration 512, loss = 0.93481172\n",
      "Iteration 513, loss = 0.93376084\n",
      "Iteration 514, loss = 0.93271143\n",
      "Iteration 515, loss = 0.93166349\n",
      "Iteration 516, loss = 0.93061702\n",
      "Iteration 517, loss = 0.92957198\n",
      "Iteration 518, loss = 0.92852839\n",
      "Iteration 519, loss = 0.92748621\n",
      "Iteration 520, loss = 0.92644545\n",
      "Iteration 521, loss = 0.92540610\n",
      "Iteration 522, loss = 0.92436814\n",
      "Iteration 523, loss = 0.92333156\n",
      "Iteration 524, loss = 0.92229635\n",
      "Iteration 525, loss = 0.92126251\n",
      "Iteration 526, loss = 0.92023001\n",
      "Iteration 527, loss = 0.91919886\n",
      "Iteration 528, loss = 0.91816904\n",
      "Iteration 529, loss = 0.91714054\n",
      "Iteration 530, loss = 0.91611336\n",
      "Iteration 531, loss = 0.91508747\n",
      "Iteration 532, loss = 0.91406288\n",
      "Iteration 533, loss = 0.91303957\n",
      "Iteration 534, loss = 0.91201753\n",
      "Iteration 535, loss = 0.91099676\n",
      "Iteration 536, loss = 0.90997724\n",
      "Iteration 537, loss = 0.90895896\n",
      "Iteration 538, loss = 0.90794193\n",
      "Iteration 539, loss = 0.90692611\n",
      "Iteration 540, loss = 0.90591152\n",
      "Iteration 541, loss = 0.90489813\n",
      "Iteration 542, loss = 0.90388595\n",
      "Iteration 543, loss = 0.90287495\n",
      "Iteration 544, loss = 0.90186514\n",
      "Iteration 545, loss = 0.90085650\n",
      "Iteration 546, loss = 0.89984902\n",
      "Iteration 547, loss = 0.89884271\n",
      "Iteration 548, loss = 0.89783754\n",
      "Iteration 549, loss = 0.89683351\n",
      "Iteration 550, loss = 0.89583061\n",
      "Iteration 551, loss = 0.89482884\n",
      "Iteration 552, loss = 0.89382818\n",
      "Iteration 553, loss = 0.89282863\n",
      "Iteration 554, loss = 0.89183018\n",
      "Iteration 555, loss = 0.89083282\n",
      "Iteration 556, loss = 0.88983655\n",
      "Iteration 557, loss = 0.88884135\n",
      "Iteration 558, loss = 0.88784723\n",
      "Iteration 559, loss = 0.88685416\n",
      "Iteration 560, loss = 0.88586215\n",
      "Iteration 561, loss = 0.88487119\n",
      "Iteration 562, loss = 0.88388127\n",
      "Iteration 563, loss = 0.88289239\n",
      "Iteration 564, loss = 0.88190453\n",
      "Iteration 565, loss = 0.88091769\n",
      "Iteration 566, loss = 0.87993186\n",
      "Iteration 567, loss = 0.87894704\n",
      "Iteration 568, loss = 0.87796322\n",
      "Iteration 569, loss = 0.87698040\n",
      "Iteration 570, loss = 0.87599856\n",
      "Iteration 571, loss = 0.87501770\n",
      "Iteration 572, loss = 0.87403781\n",
      "Iteration 573, loss = 0.87305890\n",
      "Iteration 574, loss = 0.87208094\n",
      "Iteration 575, loss = 0.87110395\n",
      "Iteration 576, loss = 0.87012790\n",
      "Iteration 577, loss = 0.86915280\n",
      "Iteration 578, loss = 0.86817863\n",
      "Iteration 579, loss = 0.86720540\n",
      "Iteration 580, loss = 0.86623310\n",
      "Iteration 581, loss = 0.86526172\n",
      "Iteration 582, loss = 0.86429126\n",
      "Iteration 583, loss = 0.86332171\n",
      "Iteration 584, loss = 0.86235306\n",
      "Iteration 585, loss = 0.86138532\n",
      "Iteration 586, loss = 0.86041847\n",
      "Iteration 587, loss = 0.85945251\n",
      "Iteration 588, loss = 0.85848744\n",
      "Iteration 589, loss = 0.85752325\n",
      "Iteration 590, loss = 0.85655994\n",
      "Iteration 591, loss = 0.85559750\n",
      "Iteration 592, loss = 0.85463593\n",
      "Iteration 593, loss = 0.85367522\n",
      "Iteration 594, loss = 0.85271537\n",
      "Iteration 595, loss = 0.85175638\n",
      "Iteration 596, loss = 0.85079823\n",
      "Iteration 597, loss = 0.84984094\n",
      "Iteration 598, loss = 0.84888448\n",
      "Iteration 599, loss = 0.84792886\n",
      "Iteration 600, loss = 0.84697408\n",
      "Iteration 601, loss = 0.84602012\n",
      "Iteration 602, loss = 0.84506700\n",
      "Iteration 603, loss = 0.84411469\n",
      "Iteration 604, loss = 0.84316321\n",
      "Iteration 605, loss = 0.84221254\n",
      "Iteration 606, loss = 0.84126268\n",
      "Iteration 607, loss = 0.84031364\n",
      "Iteration 608, loss = 0.83936540\n",
      "Iteration 609, loss = 0.83841796\n",
      "Iteration 610, loss = 0.83747132\n",
      "Iteration 611, loss = 0.83652548\n",
      "Iteration 612, loss = 0.83558043\n",
      "Iteration 613, loss = 0.83463617\n",
      "Iteration 614, loss = 0.83369269\n",
      "Iteration 615, loss = 0.83275001\n",
      "Iteration 616, loss = 0.83180810\n",
      "Iteration 617, loss = 0.83086698\n",
      "Iteration 618, loss = 0.82992663\n",
      "Iteration 619, loss = 0.82898705\n",
      "Iteration 620, loss = 0.82804825\n",
      "Iteration 621, loss = 0.82711021\n",
      "Iteration 622, loss = 0.82617295\n",
      "Iteration 623, loss = 0.82523644\n",
      "Iteration 624, loss = 0.82430071\n",
      "Iteration 625, loss = 0.82336573\n",
      "Iteration 626, loss = 0.82243151\n",
      "Iteration 627, loss = 0.82149804\n",
      "Iteration 628, loss = 0.82056533\n",
      "Iteration 629, loss = 0.81963338\n",
      "Iteration 630, loss = 0.81870217\n",
      "Iteration 631, loss = 0.81777172\n",
      "Iteration 632, loss = 0.81684201\n",
      "Iteration 633, loss = 0.81591305\n",
      "Iteration 634, loss = 0.81498483\n",
      "Iteration 635, loss = 0.81405736\n",
      "Iteration 636, loss = 0.81313062\n",
      "Iteration 637, loss = 0.81220463\n",
      "Iteration 638, loss = 0.81127938\n",
      "Iteration 639, loss = 0.81035486\n",
      "Iteration 640, loss = 0.80943108\n",
      "Iteration 641, loss = 0.80850804\n",
      "Iteration 642, loss = 0.80758573\n",
      "Iteration 643, loss = 0.80666416\n",
      "Iteration 644, loss = 0.80574331\n",
      "Iteration 645, loss = 0.80482320\n",
      "Iteration 646, loss = 0.80390382\n",
      "Iteration 647, loss = 0.80298517\n",
      "Iteration 648, loss = 0.80206725\n",
      "Iteration 649, loss = 0.80115006\n",
      "Iteration 650, loss = 0.80023360\n",
      "Iteration 651, loss = 0.79931786\n",
      "Iteration 652, loss = 0.79840286\n",
      "Iteration 653, loss = 0.79748857\n",
      "Iteration 654, loss = 0.79657502\n",
      "Iteration 655, loss = 0.79566219\n",
      "Iteration 656, loss = 0.79475009\n",
      "Iteration 657, loss = 0.79383871\n",
      "Iteration 658, loss = 0.79292805\n",
      "Iteration 659, loss = 0.79201813\n",
      "Iteration 660, loss = 0.79110893\n",
      "Iteration 661, loss = 0.79020045\n",
      "Iteration 662, loss = 0.78929270\n",
      "Iteration 663, loss = 0.78838567\n",
      "Iteration 664, loss = 0.78747937\n",
      "Iteration 665, loss = 0.78657379\n",
      "Iteration 666, loss = 0.78566894\n",
      "Iteration 667, loss = 0.78476482\n",
      "Iteration 668, loss = 0.78386142\n",
      "Iteration 669, loss = 0.78295875\n",
      "Iteration 670, loss = 0.78205681\n",
      "Iteration 671, loss = 0.78115559\n",
      "Iteration 672, loss = 0.78025511\n",
      "Iteration 673, loss = 0.77935535\n",
      "Iteration 674, loss = 0.77845632\n",
      "Iteration 675, loss = 0.77755802\n",
      "Iteration 676, loss = 0.77666045\n",
      "Iteration 677, loss = 0.77576361\n",
      "Iteration 678, loss = 0.77486751\n",
      "Iteration 679, loss = 0.77397213\n",
      "Iteration 680, loss = 0.77307749\n",
      "Iteration 681, loss = 0.77218359\n",
      "Iteration 682, loss = 0.77129042\n",
      "Iteration 683, loss = 0.77039799\n",
      "Iteration 684, loss = 0.76950629\n",
      "Iteration 685, loss = 0.76861534\n",
      "Iteration 686, loss = 0.76772512\n",
      "Iteration 687, loss = 0.76683565\n",
      "Iteration 688, loss = 0.76594691\n",
      "Iteration 689, loss = 0.76505892\n",
      "Iteration 690, loss = 0.76417168\n",
      "Iteration 691, loss = 0.76328518\n",
      "Iteration 692, loss = 0.76239942\n",
      "Iteration 693, loss = 0.76151442\n",
      "Iteration 694, loss = 0.76063016\n",
      "Iteration 695, loss = 0.75974666\n",
      "Iteration 696, loss = 0.75886391\n",
      "Iteration 697, loss = 0.75798191\n",
      "Iteration 698, loss = 0.75710067\n",
      "Iteration 699, loss = 0.75622019\n",
      "Iteration 700, loss = 0.75534046\n",
      "Iteration 701, loss = 0.75446150\n",
      "Iteration 702, loss = 0.75358330\n",
      "Iteration 703, loss = 0.75270586\n",
      "Iteration 704, loss = 0.75182918\n",
      "Iteration 705, loss = 0.75095328\n",
      "Iteration 706, loss = 0.75007814\n",
      "Iteration 707, loss = 0.74920378\n",
      "Iteration 708, loss = 0.74833018\n",
      "Iteration 709, loss = 0.74745737\n",
      "Iteration 710, loss = 0.74658533\n",
      "Iteration 711, loss = 0.74571406\n",
      "Iteration 712, loss = 0.74484358\n",
      "Iteration 713, loss = 0.74397388\n",
      "Iteration 714, loss = 0.74310496\n",
      "Iteration 715, loss = 0.74223683\n",
      "Iteration 716, loss = 0.74136949\n",
      "Iteration 717, loss = 0.74050294\n",
      "Iteration 718, loss = 0.73963719\n",
      "Iteration 719, loss = 0.73877222\n",
      "Iteration 720, loss = 0.73790806\n",
      "Iteration 721, loss = 0.73704469\n",
      "Iteration 722, loss = 0.73618213\n",
      "Iteration 723, loss = 0.73532036\n",
      "Iteration 724, loss = 0.73445941\n",
      "Iteration 725, loss = 0.73359926\n",
      "Iteration 726, loss = 0.73273992\n",
      "Iteration 727, loss = 0.73188140\n",
      "Iteration 728, loss = 0.73102369\n",
      "Iteration 729, loss = 0.73016680\n",
      "Iteration 730, loss = 0.72931073\n",
      "Iteration 731, loss = 0.72845548\n",
      "Iteration 732, loss = 0.72760105\n",
      "Iteration 733, loss = 0.72674745\n",
      "Iteration 734, loss = 0.72589468\n",
      "Iteration 735, loss = 0.72504275\n",
      "Iteration 736, loss = 0.72419164\n",
      "Iteration 737, loss = 0.72334138\n",
      "Iteration 738, loss = 0.72249195\n",
      "Iteration 739, loss = 0.72164337\n",
      "Iteration 740, loss = 0.72079562\n",
      "Iteration 741, loss = 0.71994873\n",
      "Iteration 742, loss = 0.71910269\n",
      "Iteration 743, loss = 0.71825749\n",
      "Iteration 744, loss = 0.71741316\n",
      "Iteration 745, loss = 0.71656968\n",
      "Iteration 746, loss = 0.71572706\n",
      "Iteration 747, loss = 0.71488530\n",
      "Iteration 748, loss = 0.71404441\n",
      "Iteration 749, loss = 0.71320438\n",
      "Iteration 750, loss = 0.71236523\n",
      "Iteration 751, loss = 0.71152695\n",
      "Iteration 752, loss = 0.71068954\n",
      "Iteration 753, loss = 0.70985301\n",
      "Iteration 754, loss = 0.70901737\n",
      "Iteration 755, loss = 0.70818260\n",
      "Iteration 756, loss = 0.70734873\n",
      "Iteration 757, loss = 0.70651574\n",
      "Iteration 758, loss = 0.70568364\n",
      "Iteration 759, loss = 0.70485244\n",
      "Iteration 760, loss = 0.70402214\n",
      "Iteration 761, loss = 0.70319274\n",
      "Iteration 762, loss = 0.70236423\n",
      "Iteration 763, loss = 0.70153664\n",
      "Iteration 764, loss = 0.70070995\n",
      "Iteration 765, loss = 0.69988417\n",
      "Iteration 766, loss = 0.69905931\n",
      "Iteration 767, loss = 0.69823537\n",
      "Iteration 768, loss = 0.69741234\n",
      "Iteration 769, loss = 0.69659023\n",
      "Iteration 770, loss = 0.69576905\n",
      "Iteration 771, loss = 0.69494880\n",
      "Iteration 772, loss = 0.69412947\n",
      "Iteration 773, loss = 0.69331108\n",
      "Iteration 774, loss = 0.69249363\n",
      "Iteration 775, loss = 0.69167711\n",
      "Iteration 776, loss = 0.69086153\n",
      "Iteration 777, loss = 0.69004690\n",
      "Iteration 778, loss = 0.68923322\n",
      "Iteration 779, loss = 0.68842048\n",
      "Iteration 780, loss = 0.68760869\n",
      "Iteration 781, loss = 0.68679786\n",
      "Iteration 782, loss = 0.68598799\n",
      "Iteration 783, loss = 0.68517908\n",
      "Iteration 784, loss = 0.68437113\n",
      "Iteration 785, loss = 0.68356414\n",
      "Iteration 786, loss = 0.68275812\n",
      "Iteration 787, loss = 0.68195308\n",
      "Iteration 788, loss = 0.68114901\n",
      "Iteration 789, loss = 0.68034591\n",
      "Iteration 790, loss = 0.67954379\n",
      "Iteration 791, loss = 0.67874266\n",
      "Iteration 792, loss = 0.67794250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 793, loss = 0.67714334\n",
      "Iteration 794, loss = 0.67634516\n",
      "Iteration 795, loss = 0.67554798\n",
      "Iteration 796, loss = 0.67475179\n",
      "Iteration 797, loss = 0.67395659\n",
      "Iteration 798, loss = 0.67316240\n",
      "Iteration 799, loss = 0.67236921\n",
      "Iteration 800, loss = 0.67157703\n",
      "Iteration 801, loss = 0.67078585\n",
      "Iteration 802, loss = 0.66999568\n",
      "Iteration 803, loss = 0.66920652\n",
      "Iteration 804, loss = 0.66841838\n",
      "Iteration 805, loss = 0.66763126\n",
      "Iteration 806, loss = 0.66684516\n",
      "Iteration 807, loss = 0.66606008\n",
      "Iteration 808, loss = 0.66527602\n",
      "Iteration 809, loss = 0.66449299\n",
      "Iteration 810, loss = 0.66371099\n",
      "Iteration 811, loss = 0.66293003\n",
      "Iteration 812, loss = 0.66215010\n",
      "Iteration 813, loss = 0.66137120\n",
      "Iteration 814, loss = 0.66059335\n",
      "Iteration 815, loss = 0.65981653\n",
      "Iteration 816, loss = 0.65904076\n",
      "Iteration 817, loss = 0.65826604\n",
      "Iteration 818, loss = 0.65749237\n",
      "Iteration 819, loss = 0.65671974\n",
      "Iteration 820, loss = 0.65594817\n",
      "Iteration 821, loss = 0.65517766\n",
      "Iteration 822, loss = 0.65440820\n",
      "Iteration 823, loss = 0.65363981\n",
      "Iteration 824, loss = 0.65287247\n",
      "Iteration 825, loss = 0.65210620\n",
      "Iteration 826, loss = 0.65134100\n",
      "Iteration 827, loss = 0.65057687\n",
      "Iteration 828, loss = 0.64981380\n",
      "Iteration 829, loss = 0.64905181\n",
      "Iteration 830, loss = 0.64829090\n",
      "Iteration 831, loss = 0.64753106\n",
      "Iteration 832, loss = 0.64677230\n",
      "Iteration 833, loss = 0.64601463\n",
      "Iteration 834, loss = 0.64525803\n",
      "Iteration 835, loss = 0.64450252\n",
      "Iteration 836, loss = 0.64374810\n",
      "Iteration 837, loss = 0.64299477\n",
      "Iteration 838, loss = 0.64224253\n",
      "Iteration 839, loss = 0.64149138\n",
      "Iteration 840, loss = 0.64074133\n",
      "Iteration 841, loss = 0.63999237\n",
      "Iteration 842, loss = 0.63924451\n",
      "Iteration 843, loss = 0.63849776\n",
      "Iteration 844, loss = 0.63775210\n",
      "Iteration 845, loss = 0.63700755\n",
      "Iteration 846, loss = 0.63626411\n",
      "Iteration 847, loss = 0.63552177\n",
      "Iteration 848, loss = 0.63478054\n",
      "Iteration 849, loss = 0.63404042\n",
      "Iteration 850, loss = 0.63330142\n",
      "Iteration 851, loss = 0.63256353\n",
      "Iteration 852, loss = 0.63182676\n",
      "Iteration 853, loss = 0.63109110\n",
      "Iteration 854, loss = 0.63035656\n",
      "Iteration 855, loss = 0.62962314\n",
      "Iteration 856, loss = 0.62889085\n",
      "Iteration 857, loss = 0.62815968\n",
      "Iteration 858, loss = 0.62742963\n",
      "Iteration 859, loss = 0.62670071\n",
      "Iteration 860, loss = 0.62597292\n",
      "Iteration 861, loss = 0.62524626\n",
      "Iteration 862, loss = 0.62452073\n",
      "Iteration 863, loss = 0.62379633\n",
      "Iteration 864, loss = 0.62307306\n",
      "Iteration 865, loss = 0.62235093\n",
      "Iteration 866, loss = 0.62162994\n",
      "Iteration 867, loss = 0.62091008\n",
      "Iteration 868, loss = 0.62019136\n",
      "Iteration 869, loss = 0.61947378\n",
      "Iteration 870, loss = 0.61875734\n",
      "Iteration 871, loss = 0.61804205\n",
      "Iteration 872, loss = 0.61732790\n",
      "Iteration 873, loss = 0.61661489\n",
      "Iteration 874, loss = 0.61590303\n",
      "Iteration 875, loss = 0.61519231\n",
      "Iteration 876, loss = 0.61448274\n",
      "Iteration 877, loss = 0.61377432\n",
      "Iteration 878, loss = 0.61306705\n",
      "Iteration 879, loss = 0.61236093\n",
      "Iteration 880, loss = 0.61165596\n",
      "Iteration 881, loss = 0.61095215\n",
      "Iteration 882, loss = 0.61024948\n",
      "Iteration 883, loss = 0.60954798\n",
      "Iteration 884, loss = 0.60884762\n",
      "Iteration 885, loss = 0.60814842\n",
      "Iteration 886, loss = 0.60745038\n",
      "Iteration 887, loss = 0.60675350\n",
      "Iteration 888, loss = 0.60605777\n",
      "Iteration 889, loss = 0.60536321\n",
      "Iteration 890, loss = 0.60466980\n",
      "Iteration 891, loss = 0.60397755\n",
      "Iteration 892, loss = 0.60328646\n",
      "Iteration 893, loss = 0.60259654\n",
      "Iteration 894, loss = 0.60190777\n",
      "Iteration 895, loss = 0.60122017\n",
      "Iteration 896, loss = 0.60053373\n",
      "Iteration 897, loss = 0.59984846\n",
      "Iteration 898, loss = 0.59916434\n",
      "Iteration 899, loss = 0.59848140\n",
      "Iteration 900, loss = 0.59779961\n",
      "Iteration 901, loss = 0.59711900\n",
      "Iteration 902, loss = 0.59643954\n",
      "Iteration 903, loss = 0.59576126\n",
      "Iteration 904, loss = 0.59508414\n",
      "Iteration 905, loss = 0.59440818\n",
      "Iteration 906, loss = 0.59373340\n",
      "Iteration 907, loss = 0.59305977\n",
      "Iteration 908, loss = 0.59238732\n",
      "Iteration 909, loss = 0.59171604\n",
      "Iteration 910, loss = 0.59104592\n",
      "Iteration 911, loss = 0.59037697\n",
      "Iteration 912, loss = 0.58970918\n",
      "Iteration 913, loss = 0.58904257\n",
      "Iteration 914, loss = 0.58837712\n",
      "Iteration 915, loss = 0.58771284\n",
      "Iteration 916, loss = 0.58704973\n",
      "Iteration 917, loss = 0.58638779\n",
      "Iteration 918, loss = 0.58572701\n",
      "Iteration 919, loss = 0.58506740\n",
      "Iteration 920, loss = 0.58440896\n",
      "Iteration 921, loss = 0.58375169\n",
      "Iteration 922, loss = 0.58309558\n",
      "Iteration 923, loss = 0.58244065\n",
      "Iteration 924, loss = 0.58178687\n",
      "Iteration 925, loss = 0.58113427\n",
      "Iteration 926, loss = 0.58048283\n",
      "Iteration 927, loss = 0.57983256\n",
      "Iteration 928, loss = 0.57918346\n",
      "Iteration 929, loss = 0.57853552\n",
      "Iteration 930, loss = 0.57788874\n",
      "Iteration 931, loss = 0.57724314\n",
      "Iteration 932, loss = 0.57659869\n",
      "Iteration 933, loss = 0.57595541\n",
      "Iteration 934, loss = 0.57531330\n",
      "Iteration 935, loss = 0.57467235\n",
      "Iteration 936, loss = 0.57403256\n",
      "Iteration 937, loss = 0.57339393\n",
      "Iteration 938, loss = 0.57275647\n",
      "Iteration 939, loss = 0.57212016\n",
      "Iteration 940, loss = 0.57148502\n",
      "Iteration 941, loss = 0.57085104\n",
      "Iteration 942, loss = 0.57021822\n",
      "Iteration 943, loss = 0.56958655\n",
      "Iteration 944, loss = 0.56895605\n",
      "Iteration 945, loss = 0.56832670\n",
      "Iteration 946, loss = 0.56769851\n",
      "Iteration 947, loss = 0.56707148\n",
      "Iteration 948, loss = 0.56644560\n",
      "Iteration 949, loss = 0.56582087\n",
      "Iteration 950, loss = 0.56519730\n",
      "Iteration 951, loss = 0.56457489\n",
      "Iteration 952, loss = 0.56395362\n",
      "Iteration 953, loss = 0.56333351\n",
      "Iteration 954, loss = 0.56271455\n",
      "Iteration 955, loss = 0.56209674\n",
      "Iteration 956, loss = 0.56148007\n",
      "Iteration 957, loss = 0.56086456\n",
      "Iteration 958, loss = 0.56025019\n",
      "Iteration 959, loss = 0.55963696\n",
      "Iteration 960, loss = 0.55902489\n",
      "Iteration 961, loss = 0.55841395\n",
      "Iteration 962, loss = 0.55780416\n",
      "Iteration 963, loss = 0.55719551\n",
      "Iteration 964, loss = 0.55658800\n",
      "Iteration 965, loss = 0.55598163\n",
      "Iteration 966, loss = 0.55537640\n",
      "Iteration 967, loss = 0.55477231\n",
      "Iteration 968, loss = 0.55416936\n",
      "Iteration 969, loss = 0.55356753\n",
      "Iteration 970, loss = 0.55296685\n",
      "Iteration 971, loss = 0.55236729\n",
      "Iteration 972, loss = 0.55176887\n",
      "Iteration 973, loss = 0.55117158\n",
      "Iteration 974, loss = 0.55057541\n",
      "Iteration 975, loss = 0.54998038\n",
      "Iteration 976, loss = 0.54938647\n",
      "Iteration 977, loss = 0.54879369\n",
      "Iteration 978, loss = 0.54820203\n",
      "Iteration 979, loss = 0.54761149\n",
      "Iteration 980, loss = 0.54702207\n",
      "Iteration 981, loss = 0.54643377\n",
      "Iteration 982, loss = 0.54584660\n",
      "Iteration 983, loss = 0.54526054\n",
      "Iteration 984, loss = 0.54467559\n",
      "Iteration 985, loss = 0.54409176\n",
      "Iteration 986, loss = 0.54350904\n",
      "Iteration 987, loss = 0.54292743\n",
      "Iteration 988, loss = 0.54234694\n",
      "Iteration 989, loss = 0.54176755\n",
      "Iteration 990, loss = 0.54118926\n",
      "Iteration 991, loss = 0.54061209\n",
      "Iteration 992, loss = 0.54003601\n",
      "Iteration 993, loss = 0.53946104\n",
      "Iteration 994, loss = 0.53888717\n",
      "Iteration 995, loss = 0.53831440\n",
      "Iteration 996, loss = 0.53774272\n",
      "Iteration 997, loss = 0.53717214\n",
      "Iteration 998, loss = 0.53660266\n",
      "Iteration 999, loss = 0.53603427\n",
      "Iteration 1000, loss = 0.53546697\n",
      "Iteration 1001, loss = 0.53490075\n",
      "Iteration 1002, loss = 0.53433563\n",
      "Iteration 1003, loss = 0.53377159\n",
      "Iteration 1004, loss = 0.53320864\n",
      "Iteration 1005, loss = 0.53264677\n",
      "Iteration 1006, loss = 0.53208597\n",
      "Iteration 1007, loss = 0.53152626\n",
      "Iteration 1008, loss = 0.53096763\n",
      "Iteration 1009, loss = 0.53041007\n",
      "Iteration 1010, loss = 0.52985358\n",
      "Iteration 1011, loss = 0.52929817\n",
      "Iteration 1012, loss = 0.52874383\n",
      "Iteration 1013, loss = 0.52819055\n",
      "Iteration 1014, loss = 0.52763834\n",
      "Iteration 1015, loss = 0.52708720\n",
      "Iteration 1016, loss = 0.52653712\n",
      "Iteration 1017, loss = 0.52598810\n",
      "Iteration 1018, loss = 0.52544014\n",
      "Iteration 1019, loss = 0.52489324\n",
      "Iteration 1020, loss = 0.52434739\n",
      "Iteration 1021, loss = 0.52380260\n",
      "Iteration 1022, loss = 0.52325886\n",
      "Iteration 1023, loss = 0.52271617\n",
      "Iteration 1024, loss = 0.52217453\n",
      "Iteration 1025, loss = 0.52163393\n",
      "Iteration 1026, loss = 0.52109438\n",
      "Iteration 1027, loss = 0.52055587\n",
      "Iteration 1028, loss = 0.52001840\n",
      "Iteration 1029, loss = 0.51948197\n",
      "Iteration 1030, loss = 0.51894657\n",
      "Iteration 1031, loss = 0.51841221\n",
      "Iteration 1032, loss = 0.51787889\n",
      "Iteration 1033, loss = 0.51734659\n",
      "Iteration 1034, loss = 0.51681533\n",
      "Iteration 1035, loss = 0.51628508\n",
      "Iteration 1036, loss = 0.51575587\n",
      "Iteration 1037, loss = 0.51522768\n",
      "Iteration 1038, loss = 0.51470050\n",
      "Iteration 1039, loss = 0.51417435\n",
      "Iteration 1040, loss = 0.51364922\n",
      "Iteration 1041, loss = 0.51312510\n",
      "Iteration 1042, loss = 0.51260199\n",
      "Iteration 1043, loss = 0.51207989\n",
      "Iteration 1044, loss = 0.51155880\n",
      "Iteration 1045, loss = 0.51103872\n",
      "Iteration 1046, loss = 0.51051964\n",
      "Iteration 1047, loss = 0.51000157\n",
      "Iteration 1048, loss = 0.50948450\n",
      "Iteration 1049, loss = 0.50896842\n",
      "Iteration 1050, loss = 0.50845334\n",
      "Iteration 1051, loss = 0.50793926\n",
      "Iteration 1052, loss = 0.50742617\n",
      "Iteration 1053, loss = 0.50691407\n",
      "Iteration 1054, loss = 0.50640296\n",
      "Iteration 1055, loss = 0.50589283\n",
      "Iteration 1056, loss = 0.50538369\n",
      "Iteration 1057, loss = 0.50487553\n",
      "Iteration 1058, loss = 0.50436836\n",
      "Iteration 1059, loss = 0.50386216\n",
      "Iteration 1060, loss = 0.50335693\n",
      "Iteration 1061, loss = 0.50285268\n",
      "Iteration 1062, loss = 0.50234940\n",
      "Iteration 1063, loss = 0.50184710\n",
      "Iteration 1064, loss = 0.50134576\n",
      "Iteration 1065, loss = 0.50084538\n",
      "Iteration 1066, loss = 0.50034597\n",
      "Iteration 1067, loss = 0.49984752\n",
      "Iteration 1068, loss = 0.49935003\n",
      "Iteration 1069, loss = 0.49885350\n",
      "Iteration 1070, loss = 0.49835792\n",
      "Iteration 1071, loss = 0.49786330\n",
      "Iteration 1072, loss = 0.49736963\n",
      "Iteration 1073, loss = 0.49687690\n",
      "Iteration 1074, loss = 0.49638513\n",
      "Iteration 1075, loss = 0.49589430\n",
      "Iteration 1076, loss = 0.49540441\n",
      "Iteration 1077, loss = 0.49491546\n",
      "Iteration 1078, loss = 0.49442745\n",
      "Iteration 1079, loss = 0.49394038\n",
      "Iteration 1080, loss = 0.49345424\n",
      "Iteration 1081, loss = 0.49296903\n",
      "Iteration 1082, loss = 0.49248476\n",
      "Iteration 1083, loss = 0.49200141\n",
      "Iteration 1084, loss = 0.49151899\n",
      "Iteration 1085, loss = 0.49103749\n",
      "Iteration 1086, loss = 0.49055691\n",
      "Iteration 1087, loss = 0.49007726\n",
      "Iteration 1088, loss = 0.48959852\n",
      "Iteration 1089, loss = 0.48912070\n",
      "Iteration 1090, loss = 0.48864379\n",
      "Iteration 1091, loss = 0.48816779\n",
      "Iteration 1092, loss = 0.48769270\n",
      "Iteration 1093, loss = 0.48721852\n",
      "Iteration 1094, loss = 0.48674525\n",
      "Iteration 1095, loss = 0.48627288\n",
      "Iteration 1096, loss = 0.48580141\n",
      "Iteration 1097, loss = 0.48533084\n",
      "Iteration 1098, loss = 0.48486116\n",
      "Iteration 1099, loss = 0.48439238\n",
      "Iteration 1100, loss = 0.48392450\n",
      "Iteration 1101, loss = 0.48345750\n",
      "Iteration 1102, loss = 0.48299139\n",
      "Iteration 1103, loss = 0.48252617\n",
      "Iteration 1104, loss = 0.48206184\n",
      "Iteration 1105, loss = 0.48159839\n",
      "Iteration 1106, loss = 0.48113581\n",
      "Iteration 1107, loss = 0.48067412\n",
      "Iteration 1108, loss = 0.48021330\n",
      "Iteration 1109, loss = 0.47975336\n",
      "Iteration 1110, loss = 0.47929429\n",
      "Iteration 1111, loss = 0.47883609\n",
      "Iteration 1112, loss = 0.47837876\n",
      "Iteration 1113, loss = 0.47792230\n",
      "Iteration 1114, loss = 0.47746670\n",
      "Iteration 1115, loss = 0.47701196\n",
      "Iteration 1116, loss = 0.47655808\n",
      "Iteration 1117, loss = 0.47610506\n",
      "Iteration 1118, loss = 0.47565290\n",
      "Iteration 1119, loss = 0.47520159\n",
      "Iteration 1120, loss = 0.47475113\n",
      "Iteration 1121, loss = 0.47430152\n",
      "Iteration 1122, loss = 0.47385276\n",
      "Iteration 1123, loss = 0.47340485\n",
      "Iteration 1124, loss = 0.47295778\n",
      "Iteration 1125, loss = 0.47251156\n",
      "Iteration 1126, loss = 0.47206617\n",
      "Iteration 1127, loss = 0.47162163\n",
      "Iteration 1128, loss = 0.47117791\n",
      "Iteration 1129, loss = 0.47073504\n",
      "Iteration 1130, loss = 0.47029299\n",
      "Iteration 1131, loss = 0.46985178\n",
      "Iteration 1132, loss = 0.46941139\n",
      "Iteration 1133, loss = 0.46897184\n",
      "Iteration 1134, loss = 0.46853310\n",
      "Iteration 1135, loss = 0.46809519\n",
      "Iteration 1136, loss = 0.46765810\n",
      "Iteration 1137, loss = 0.46722183\n",
      "Iteration 1138, loss = 0.46678637\n",
      "Iteration 1139, loss = 0.46635173\n",
      "Iteration 1140, loss = 0.46591790\n",
      "Iteration 1141, loss = 0.46548488\n",
      "Iteration 1142, loss = 0.46505268\n",
      "Iteration 1143, loss = 0.46462127\n",
      "Iteration 1144, loss = 0.46419068\n",
      "Iteration 1145, loss = 0.46376089\n",
      "Iteration 1146, loss = 0.46333190\n",
      "Iteration 1147, loss = 0.46290370\n",
      "Iteration 1148, loss = 0.46247631\n",
      "Iteration 1149, loss = 0.46204971\n",
      "Iteration 1150, loss = 0.46162391\n",
      "Iteration 1151, loss = 0.46119889\n",
      "Iteration 1152, loss = 0.46077467\n",
      "Iteration 1153, loss = 0.46035124\n",
      "Iteration 1154, loss = 0.45992859\n",
      "Iteration 1155, loss = 0.45950673\n",
      "Iteration 1156, loss = 0.45908565\n",
      "Iteration 1157, loss = 0.45866535\n",
      "Iteration 1158, loss = 0.45824582\n",
      "Iteration 1159, loss = 0.45782708\n",
      "Iteration 1160, loss = 0.45740911\n",
      "Iteration 1161, loss = 0.45699191\n",
      "Iteration 1162, loss = 0.45657549\n",
      "Iteration 1163, loss = 0.45615984\n",
      "Iteration 1164, loss = 0.45574495\n",
      "Iteration 1165, loss = 0.45533083\n",
      "Iteration 1166, loss = 0.45491747\n",
      "Iteration 1167, loss = 0.45450488\n",
      "Iteration 1168, loss = 0.45409304\n",
      "Iteration 1169, loss = 0.45368197\n",
      "Iteration 1170, loss = 0.45327165\n",
      "Iteration 1171, loss = 0.45286209\n",
      "Iteration 1172, loss = 0.45245328\n",
      "Iteration 1173, loss = 0.45204522\n",
      "Iteration 1174, loss = 0.45163791\n",
      "Iteration 1175, loss = 0.45123135\n",
      "Iteration 1176, loss = 0.45082554\n",
      "Iteration 1177, loss = 0.45042047\n",
      "Iteration 1178, loss = 0.45001615\n",
      "Iteration 1179, loss = 0.44961256\n",
      "Iteration 1180, loss = 0.44920972\n",
      "Iteration 1181, loss = 0.44880761\n",
      "Iteration 1182, loss = 0.44840623\n",
      "Iteration 1183, loss = 0.44800560\n",
      "Iteration 1184, loss = 0.44760569\n",
      "Iteration 1185, loss = 0.44720651\n",
      "Iteration 1186, loss = 0.44680807\n",
      "Iteration 1187, loss = 0.44641035\n",
      "Iteration 1188, loss = 0.44601335\n",
      "Iteration 1189, loss = 0.44561708\n",
      "Iteration 1190, loss = 0.44522154\n",
      "Iteration 1191, loss = 0.44482671\n",
      "Iteration 1192, loss = 0.44443260\n",
      "Iteration 1193, loss = 0.44403921\n",
      "Iteration 1194, loss = 0.44364653\n",
      "Iteration 1195, loss = 0.44325457\n",
      "Iteration 1196, loss = 0.44286331\n",
      "Iteration 1197, loss = 0.44247277\n",
      "Iteration 1198, loss = 0.44208294\n",
      "Iteration 1199, loss = 0.44169381\n",
      "Iteration 1200, loss = 0.44130539\n",
      "Iteration 1201, loss = 0.44091768\n",
      "Iteration 1202, loss = 0.44053066\n",
      "Iteration 1203, loss = 0.44014435\n",
      "Iteration 1204, loss = 0.43975873\n",
      "Iteration 1205, loss = 0.43937381\n",
      "Iteration 1206, loss = 0.43898959\n",
      "Iteration 1207, loss = 0.43860606\n",
      "Iteration 1208, loss = 0.43822322\n",
      "Iteration 1209, loss = 0.43784107\n",
      "Iteration 1210, loss = 0.43745961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1211, loss = 0.43707884\n",
      "Iteration 1212, loss = 0.43669875\n",
      "Iteration 1213, loss = 0.43631935\n",
      "Iteration 1214, loss = 0.43594063\n",
      "Iteration 1215, loss = 0.43556259\n",
      "Iteration 1216, loss = 0.43518523\n",
      "Iteration 1217, loss = 0.43480855\n",
      "Iteration 1218, loss = 0.43443255\n",
      "Iteration 1219, loss = 0.43405722\n",
      "Iteration 1220, loss = 0.43368256\n",
      "Iteration 1221, loss = 0.43330857\n",
      "Iteration 1222, loss = 0.43293526\n",
      "Iteration 1223, loss = 0.43256261\n",
      "Iteration 1224, loss = 0.43219063\n",
      "Iteration 1225, loss = 0.43181931\n",
      "Iteration 1226, loss = 0.43144866\n",
      "Iteration 1227, loss = 0.43107867\n",
      "Iteration 1228, loss = 0.43070934\n",
      "Iteration 1229, loss = 0.43034067\n",
      "Iteration 1230, loss = 0.42997265\n",
      "Iteration 1231, loss = 0.42960530\n",
      "Iteration 1232, loss = 0.42923859\n",
      "Iteration 1233, loss = 0.42887254\n",
      "Iteration 1234, loss = 0.42850715\n",
      "Iteration 1235, loss = 0.42814240\n",
      "Iteration 1236, loss = 0.42777830\n",
      "Iteration 1237, loss = 0.42741484\n",
      "Iteration 1238, loss = 0.42705204\n",
      "Iteration 1239, loss = 0.42668987\n",
      "Iteration 1240, loss = 0.42632835\n",
      "Iteration 1241, loss = 0.42596747\n",
      "Iteration 1242, loss = 0.42560723\n",
      "Iteration 1243, loss = 0.42524763\n",
      "Iteration 1244, loss = 0.42488866\n",
      "Iteration 1245, loss = 0.42453033\n",
      "Iteration 1246, loss = 0.42417264\n",
      "Iteration 1247, loss = 0.42381557\n",
      "Iteration 1248, loss = 0.42345914\n",
      "Iteration 1249, loss = 0.42310333\n",
      "Iteration 1250, loss = 0.42274816\n",
      "Iteration 1251, loss = 0.42239361\n",
      "Iteration 1252, loss = 0.42203968\n",
      "Iteration 1253, loss = 0.42168638\n",
      "Iteration 1254, loss = 0.42133370\n",
      "Iteration 1255, loss = 0.42098164\n",
      "Iteration 1256, loss = 0.42063020\n",
      "Iteration 1257, loss = 0.42027938\n",
      "Iteration 1258, loss = 0.41992918\n",
      "Iteration 1259, loss = 0.41957959\n",
      "Iteration 1260, loss = 0.41923061\n",
      "Iteration 1261, loss = 0.41888225\n",
      "Iteration 1262, loss = 0.41853450\n",
      "Iteration 1263, loss = 0.41818735\n",
      "Iteration 1264, loss = 0.41784082\n",
      "Iteration 1265, loss = 0.41749489\n",
      "Iteration 1266, loss = 0.41714957\n",
      "Iteration 1267, loss = 0.41680485\n",
      "Iteration 1268, loss = 0.41646073\n",
      "Iteration 1269, loss = 0.41611722\n",
      "Iteration 1270, loss = 0.41577430\n",
      "Iteration 1271, loss = 0.41543198\n",
      "Iteration 1272, loss = 0.41509026\n",
      "Iteration 1273, loss = 0.41474914\n",
      "Iteration 1274, loss = 0.41440861\n",
      "Iteration 1275, loss = 0.41406867\n",
      "Iteration 1276, loss = 0.41372933\n",
      "Iteration 1277, loss = 0.41339057\n",
      "Iteration 1278, loss = 0.41305241\n",
      "Iteration 1279, loss = 0.41271483\n",
      "Iteration 1280, loss = 0.41237784\n",
      "Iteration 1281, loss = 0.41204143\n",
      "Iteration 1282, loss = 0.41170561\n",
      "Iteration 1283, loss = 0.41137037\n",
      "Iteration 1284, loss = 0.41103571\n",
      "Iteration 1285, loss = 0.41070164\n",
      "Iteration 1286, loss = 0.41036814\n",
      "Iteration 1287, loss = 0.41003521\n",
      "Iteration 1288, loss = 0.40970287\n",
      "Iteration 1289, loss = 0.40937109\n",
      "Iteration 1290, loss = 0.40903989\n",
      "Iteration 1291, loss = 0.40870927\n",
      "Iteration 1292, loss = 0.40837921\n",
      "Iteration 1293, loss = 0.40804972\n",
      "Iteration 1294, loss = 0.40772081\n",
      "Iteration 1295, loss = 0.40739245\n",
      "Iteration 1296, loss = 0.40706467\n",
      "Iteration 1297, loss = 0.40673745\n",
      "Iteration 1298, loss = 0.40641079\n",
      "Iteration 1299, loss = 0.40608469\n",
      "Iteration 1300, loss = 0.40575916\n",
      "Iteration 1301, loss = 0.40543418\n",
      "Iteration 1302, loss = 0.40510976\n",
      "Iteration 1303, loss = 0.40478590\n",
      "Iteration 1304, loss = 0.40446259\n",
      "Iteration 1305, loss = 0.40413984\n",
      "Iteration 1306, loss = 0.40381764\n",
      "Iteration 1307, loss = 0.40349600\n",
      "Iteration 1308, loss = 0.40317490\n",
      "Iteration 1309, loss = 0.40285435\n",
      "Iteration 1310, loss = 0.40253436\n",
      "Iteration 1311, loss = 0.40221491\n",
      "Iteration 1312, loss = 0.40189600\n",
      "Iteration 1313, loss = 0.40157764\n",
      "Iteration 1314, loss = 0.40125982\n",
      "Iteration 1315, loss = 0.40094255\n",
      "Iteration 1316, loss = 0.40062581\n",
      "Iteration 1317, loss = 0.40030962\n",
      "Iteration 1318, loss = 0.39999396\n",
      "Iteration 1319, loss = 0.39967884\n",
      "Iteration 1320, loss = 0.39936426\n",
      "Iteration 1321, loss = 0.39905021\n",
      "Iteration 1322, loss = 0.39873670\n",
      "Iteration 1323, loss = 0.39842372\n",
      "Iteration 1324, loss = 0.39811127\n",
      "Iteration 1325, loss = 0.39779935\n",
      "Iteration 1326, loss = 0.39748796\n",
      "Iteration 1327, loss = 0.39717709\n",
      "Iteration 1328, loss = 0.39686676\n",
      "Iteration 1329, loss = 0.39655695\n",
      "Iteration 1330, loss = 0.39624766\n",
      "Iteration 1331, loss = 0.39593890\n",
      "Iteration 1332, loss = 0.39563066\n",
      "Iteration 1333, loss = 0.39532294\n",
      "Iteration 1334, loss = 0.39501574\n",
      "Iteration 1335, loss = 0.39470906\n",
      "Iteration 1336, loss = 0.39440289\n",
      "Iteration 1337, loss = 0.39409725\n",
      "Iteration 1338, loss = 0.39379212\n",
      "Iteration 1339, loss = 0.39348750\n",
      "Iteration 1340, loss = 0.39318339\n",
      "Iteration 1341, loss = 0.39287980\n",
      "Iteration 1342, loss = 0.39257672\n",
      "Iteration 1343, loss = 0.39227414\n",
      "Iteration 1344, loss = 0.39197208\n",
      "Iteration 1345, loss = 0.39167052\n",
      "Iteration 1346, loss = 0.39136947\n",
      "Iteration 1347, loss = 0.39106893\n",
      "Iteration 1348, loss = 0.39076888\n",
      "Iteration 1349, loss = 0.39046935\n",
      "Iteration 1350, loss = 0.39017031\n",
      "Iteration 1351, loss = 0.38987177\n",
      "Iteration 1352, loss = 0.38957374\n",
      "Iteration 1353, loss = 0.38927620\n",
      "Iteration 1354, loss = 0.38897916\n",
      "Iteration 1355, loss = 0.38868261\n",
      "Iteration 1356, loss = 0.38838657\n",
      "Iteration 1357, loss = 0.38809101\n",
      "Iteration 1358, loss = 0.38779595\n",
      "Iteration 1359, loss = 0.38750138\n",
      "Iteration 1360, loss = 0.38720730\n",
      "Iteration 1361, loss = 0.38691371\n",
      "Iteration 1362, loss = 0.38662061\n",
      "Iteration 1363, loss = 0.38632800\n",
      "Iteration 1364, loss = 0.38603587\n",
      "Iteration 1365, loss = 0.38574423\n",
      "Iteration 1366, loss = 0.38545308\n",
      "Iteration 1367, loss = 0.38516241\n",
      "Iteration 1368, loss = 0.38487222\n",
      "Iteration 1369, loss = 0.38458251\n",
      "Iteration 1370, loss = 0.38429328\n",
      "Iteration 1371, loss = 0.38400454\n",
      "Iteration 1372, loss = 0.38371627\n",
      "Iteration 1373, loss = 0.38342847\n",
      "Iteration 1374, loss = 0.38314116\n",
      "Iteration 1375, loss = 0.38285432\n",
      "Iteration 1376, loss = 0.38256795\n",
      "Iteration 1377, loss = 0.38228205\n",
      "Iteration 1378, loss = 0.38199663\n",
      "Iteration 1379, loss = 0.38171168\n",
      "Iteration 1380, loss = 0.38142720\n",
      "Iteration 1381, loss = 0.38114319\n",
      "Iteration 1382, loss = 0.38085965\n",
      "Iteration 1383, loss = 0.38057657\n",
      "Iteration 1384, loss = 0.38029396\n",
      "Iteration 1385, loss = 0.38001181\n",
      "Iteration 1386, loss = 0.37973013\n",
      "Iteration 1387, loss = 0.37944891\n",
      "Iteration 1388, loss = 0.37916816\n",
      "Iteration 1389, loss = 0.37888786\n",
      "Iteration 1390, loss = 0.37860803\n",
      "Iteration 1391, loss = 0.37832865\n",
      "Iteration 1392, loss = 0.37804973\n",
      "Iteration 1393, loss = 0.37777127\n",
      "Iteration 1394, loss = 0.37749326\n",
      "Iteration 1395, loss = 0.37721571\n",
      "Iteration 1396, loss = 0.37693862\n",
      "Iteration 1397, loss = 0.37666198\n",
      "Iteration 1398, loss = 0.37638578\n",
      "Iteration 1399, loss = 0.37611004\n",
      "Iteration 1400, loss = 0.37583475\n",
      "Iteration 1401, loss = 0.37555991\n",
      "Iteration 1402, loss = 0.37528552\n",
      "Iteration 1403, loss = 0.37501158\n",
      "Iteration 1404, loss = 0.37473808\n",
      "Iteration 1405, loss = 0.37446503\n",
      "Iteration 1406, loss = 0.37419242\n",
      "Iteration 1407, loss = 0.37392025\n",
      "Iteration 1408, loss = 0.37364853\n",
      "Iteration 1409, loss = 0.37337725\n",
      "Iteration 1410, loss = 0.37310641\n",
      "Iteration 1411, loss = 0.37283601\n",
      "Iteration 1412, loss = 0.37256605\n",
      "Iteration 1413, loss = 0.37229652\n",
      "Iteration 1414, loss = 0.37202744\n",
      "Iteration 1415, loss = 0.37175878\n",
      "Iteration 1416, loss = 0.37149057\n",
      "Iteration 1417, loss = 0.37122279\n",
      "Iteration 1418, loss = 0.37095544\n",
      "Iteration 1419, loss = 0.37068852\n",
      "Iteration 1420, loss = 0.37042204\n",
      "Iteration 1421, loss = 0.37015598\n",
      "Iteration 1422, loss = 0.36989035\n",
      "Iteration 1423, loss = 0.36962516\n",
      "Iteration 1424, loss = 0.36936039\n",
      "Iteration 1425, loss = 0.36909605\n",
      "Iteration 1426, loss = 0.36883213\n",
      "Iteration 1427, loss = 0.36856864\n",
      "Iteration 1428, loss = 0.36830557\n",
      "Iteration 1429, loss = 0.36804293\n",
      "Iteration 1430, loss = 0.36778070\n",
      "Iteration 1431, loss = 0.36751890\n",
      "Iteration 1432, loss = 0.36725752\n",
      "Iteration 1433, loss = 0.36699656\n",
      "Iteration 1434, loss = 0.36673602\n",
      "Iteration 1435, loss = 0.36647589\n",
      "Iteration 1436, loss = 0.36621619\n",
      "Iteration 1437, loss = 0.36595690\n",
      "Iteration 1438, loss = 0.36569802\n",
      "Iteration 1439, loss = 0.36543956\n",
      "Iteration 1440, loss = 0.36518151\n",
      "Iteration 1441, loss = 0.36492387\n",
      "Iteration 1442, loss = 0.36466665\n",
      "Iteration 1443, loss = 0.36440983\n",
      "Iteration 1444, loss = 0.36415343\n",
      "Iteration 1445, loss = 0.36389743\n",
      "Iteration 1446, loss = 0.36364185\n",
      "Iteration 1447, loss = 0.36338667\n",
      "Iteration 1448, loss = 0.36313189\n",
      "Iteration 1449, loss = 0.36287753\n",
      "Iteration 1450, loss = 0.36262356\n",
      "Iteration 1451, loss = 0.36237000\n",
      "Iteration 1452, loss = 0.36211685\n",
      "Iteration 1453, loss = 0.36186409\n",
      "Iteration 1454, loss = 0.36161174\n",
      "Iteration 1455, loss = 0.36135979\n",
      "Iteration 1456, loss = 0.36110824\n",
      "Iteration 1457, loss = 0.36085708\n",
      "Iteration 1458, loss = 0.36060633\n",
      "Iteration 1459, loss = 0.36035597\n",
      "Iteration 1460, loss = 0.36010600\n",
      "Iteration 1461, loss = 0.35985644\n",
      "Iteration 1462, loss = 0.35960727\n",
      "Iteration 1463, loss = 0.35935849\n",
      "Iteration 1464, loss = 0.35911010\n",
      "Iteration 1465, loss = 0.35886211\n",
      "Iteration 1466, loss = 0.35861451\n",
      "Iteration 1467, loss = 0.35836729\n",
      "Iteration 1468, loss = 0.35812047\n",
      "Iteration 1469, loss = 0.35787404\n",
      "Iteration 1470, loss = 0.35762799\n",
      "Iteration 1471, loss = 0.35738234\n",
      "Iteration 1472, loss = 0.35713706\n",
      "Iteration 1473, loss = 0.35689218\n",
      "Iteration 1474, loss = 0.35664768\n",
      "Iteration 1475, loss = 0.35640356\n",
      "Iteration 1476, loss = 0.35615983\n",
      "Iteration 1477, loss = 0.35591648\n",
      "Iteration 1478, loss = 0.35567351\n",
      "Iteration 1479, loss = 0.35543092\n",
      "Iteration 1480, loss = 0.35518871\n",
      "Iteration 1481, loss = 0.35494688\n",
      "Iteration 1482, loss = 0.35470543\n",
      "Iteration 1483, loss = 0.35446435\n",
      "Iteration 1484, loss = 0.35422366\n",
      "Iteration 1485, loss = 0.35398334\n",
      "Iteration 1486, loss = 0.35374339\n",
      "Iteration 1487, loss = 0.35350382\n",
      "Iteration 1488, loss = 0.35326462\n",
      "Iteration 1489, loss = 0.35302580\n",
      "Iteration 1490, loss = 0.35278734\n",
      "Iteration 1491, loss = 0.35254926\n",
      "Iteration 1492, loss = 0.35231155\n",
      "Iteration 1493, loss = 0.35207421\n",
      "Iteration 1494, loss = 0.35183724\n",
      "Iteration 1495, loss = 0.35160064\n",
      "Iteration 1496, loss = 0.35136440\n",
      "Iteration 1497, loss = 0.35112854\n",
      "Iteration 1498, loss = 0.35089303\n",
      "Iteration 1499, loss = 0.35065790\n",
      "Iteration 1500, loss = 0.35042312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leninml/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1500, momentum=0.95,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_iris.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluando el desempeño de entrenamiento de la Red Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_iris.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluando el desempeño de la Red Iris en el dataset de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_iris.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dibujando la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cost_function = red_iris.loss_curve_\n",
    "plt.figure()\n",
    "plt.plot(cost_function)\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
